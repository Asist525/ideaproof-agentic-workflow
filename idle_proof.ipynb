{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kVNAeg6DUMdB"
      },
      "outputs": [],
      "source": [
        "#!pip -q install -U openai==1.81.0 langgraph langchain-upstage langchain-community chromadb transformers python-dotenv pydantic rich\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "f0YKeS2CU3CR"
      },
      "outputs": [],
      "source": [
        "#!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9qoJpNkmWqxG"
      },
      "outputs": [],
      "source": [
        "#!pip -q install -U trafilatura readability-lxml beautifulsoup4 lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "pGRODEH4Usah",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "outputId": "133e93c4-47a4-4750-9a94-0ab180321e8d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OpenAIError",
          "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4095779475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mSERPER_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SERPER_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUPSTAGE_API_KEY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://api.upstage.ai/v1/solar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mchroma_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPersistentClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Upstage/SOLAR-10.7B-Chat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OPENAI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             raise OpenAIError(\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0;34m\"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             )\n",
            "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
          ]
        }
      ],
      "source": [
        "import os, json, time, math, re, hashlib, textwrap\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Tuple, TypedDict, Literal\n",
        "\n",
        "import requests\n",
        "from dotenv import load_dotenv\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "\n",
        "from openai import OpenAI\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import chromadb\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings, PersistentClient\n",
        "\n",
        "from rich import print as rprint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZI1VTZaWW7Y"
      },
      "source": [
        "## 1) API Keys & Clients\n",
        "\n",
        "- `UPSTAGE_API_KEY`, `SERPER_API_KEY`는 **이미 등록되어 있고 변수명도 동일**하다고 했으니 그대로 씁니다.\n",
        "- 모델 라인업(예시):  \n",
        "  - Solar: `solar-pro2-250909`  \n",
        "  - Document Parse: `document-parse-250618`  \n",
        "  - Embedding: `solar-embedding-1-large-query`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "\n",
        "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")\n",
        "SERPER_API_KEY  = os.getenv(\"SERPER_API_KEY\")\n",
        "DART_API_KEY    = os.getenv(\"DART_API_KEY\")  # 선택(없어도 동작)\n",
        "\n",
        "# Colab userdata (선택)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    UPSTAGE_API_KEY = UPSTAGE_API_KEY or userdata.get(\"UPSTAGE_API_KEY\")\n",
        "    SERPER_API_KEY  = SERPER_API_KEY  or userdata.get(\"SERPER_API_KEY\")\n",
        "    DART_API_KEY    = DART_API_KEY    or userdata.get(\"DART_API_KEY\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "assert UPSTAGE_API_KEY, \"UPSTAGE_API_KEY not found\"\n",
        "assert SERPER_API_KEY, \"SERPER_API_KEY not found\"\n",
        "\n",
        "client = OpenAI(base_url=\"https://api.upstage.ai/v1\", api_key=UPSTAGE_API_KEY)\n",
        "\n",
        "# 토큰 추정용 (HF 토크나이저는 model_max_length=4096 경고가 뜨는 경우가 많아서 무력화)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"upstage/solar-pro-preview-instruct\")\n",
        "tokenizer.model_max_length = 1_000_000\n",
        "\n",
        "# Solar-Pro2 컨텍스트(≈64K) 안전값\n",
        "MAX_CONTEXT_LIMIT = 65000\n",
        "\n",
        "# Vector DB\n",
        "CHROMA_PATH = \"./chroma_db_ideaproof\"\n",
        "chroma_client = PersistentClient(path=CHROMA_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310,
          "referenced_widgets": [
            "8bdf38cf2f894b4e9b99c1e76cd2c4dc",
            "9ffa42d9a8c04e0ebf6b32faf3499249",
            "f15d7c3364c640b5802dcd7fd51a1077",
            "cc316a8081504ccba2b83fd1279c72a6",
            "7b1fc055802548f6ae62daa869ff22f2",
            "a61c8336c2974892be036797e8c06bf2",
            "ce1e16479a2f4bc781aff839fbf22e28",
            "41af3d55107a460586458be83dc35588",
            "47778ab100c84431a79905af78fd26e3",
            "1257b01f47d44fa19fe12fa46f362382",
            "6810648e6bd4446d891fc55e390b9698",
            "106387fd8fb642fd981b560a975f5893",
            "050fa80c0510407a8e97a64970e7b15d",
            "599bb2b216f24a62b09239dbf70c5304",
            "2f9349df7e6649c583675a9ac3f7b929",
            "73e4bf219dd74b2d8edf2931edb2a993",
            "6c029e0fcc9b4548857dda04d697e44b",
            "fa5c56c7a16c42e3aebc4e093ea2c8cb",
            "3fd969898b3143bebb47c441d67d261c",
            "4e552a8507d947e3a8cb203168734eda",
            "e2cad387267245ef80ce6d93537c61f6",
            "520eeb66026d439087022f75440ed023",
            "291767cbc913437095f499c9d65d0aba",
            "b0ad2d6f952147b9b718f2f0e7e503fe",
            "07f9c93dcd7147b9991c72e1435caaad",
            "f46bd4e68e274c57a3d4932f2a31fb56",
            "11a7b7e22bf64b7cab99c023ad7863a0",
            "6e6088d8ad5f4dfa9a68c016bccc514a",
            "fb77cbc6b55d4a05ae47c0430d5743de",
            "f9419e481d774662af7919cd0ee2cf5a",
            "b4b2df44258b463ab2d05a40f58f9ac6",
            "ea67c00cc6c840c2bc52364640fcf42a",
            "b27235c9e5de43158f99753f40329865",
            "6a0f5dfdede5434e9599dd97fb993374",
            "619c6ce072644ade91d1583fa69aef07",
            "edd0431253234612b2374d4e6359b479",
            "48a4c3f78ad34aa3a490fa571cd119d7",
            "c52e7f326cd24114ac4525f72cd5b5ed",
            "d52049b153f34e71bac569f797a7a541",
            "5b2808fc518045239977a851b14c6d24",
            "82a741dba2c34d848d15e9bf65b24fbd",
            "a1274498cd6c4c25bc5f7672aa34790d",
            "559748f2c9fd4ee6ab379a0145859b83",
            "b1869747fe734fb2b6441e8950bd3519",
            "02b40eceb6ff4c7e8b864d1f1b31d213",
            "81ed446696884dbebd3256e1eb82b1d0",
            "885efc1add764621b687abb001479b72",
            "43487b9ac7ec4c69b1632155b4f427ee",
            "5c242ab75e944b9394ef7bb01a120439",
            "d81f41cac8cb432bbf5117198f119d8c",
            "4b014a87c5bc46a28da41e8573451d31",
            "1a6f91edb59a4075ab37cbfa485c0292",
            "cfa9e44224e542b8a8582d8842fcd5a3",
            "c26d68b3dc7244a2b6952e40fafed87c",
            "3a9bfd7d43b14a9cbbdb13606e8a5f90"
          ]
        },
        "id": "EMfvZj6kXitd",
        "outputId": "784adcbb-a5ce-4bb9-8099-35a02e5ed6e6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bdf38cf2f894b4e9b99c1e76cd2c4dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "106387fd8fb642fd981b560a975f5893"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "291767cbc913437095f499c9d65d0aba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a0f5dfdede5434e9599dd97fb993374"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/575 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02b40eceb6ff4c7e8b864d1f1b31d213"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kjgZfRoWWXoO"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "def function_to_schema(func) -> dict:\n",
        "    sig = inspect.signature(func)\n",
        "    props = {}\n",
        "    required = []\n",
        "    for name, param in sig.parameters.items():\n",
        "        if name in (\"self\",):\n",
        "            continue\n",
        "        ann = param.annotation\n",
        "        jtype = \"string\"\n",
        "        if ann in (int,):\n",
        "            jtype = \"integer\"\n",
        "        elif ann in (float,):\n",
        "            jtype = \"number\"\n",
        "        elif ann in (bool,):\n",
        "            jtype = \"boolean\"\n",
        "        elif ann in (list, List):\n",
        "            jtype = \"array\"\n",
        "        elif ann in (dict, Dict):\n",
        "            jtype = \"object\"\n",
        "        props[name] = {\"type\": jtype}\n",
        "        if param.default is inspect._empty:\n",
        "            required.append(name)\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": func.__name__,\n",
        "            \"description\": (func.__doc__ or \"\").strip(),\n",
        "            \"parameters\": {\"type\": \"object\", \"properties\": props, \"required\": required}\n",
        "        }\n",
        "    }\n",
        "\n",
        "def truncate_tokens_if_needed(tokenizer, agent_instructions, messages, content, max_token_limit=None):\n",
        "    \"\"\"\n",
        "    - base가 이미 limit을 넘으면(히스토리 과다) 에러 내지 말고 내용을 최소화해서 계속 진행\n",
        "    - content가 넘치면 content만 잘라서 limit 안으로 넣기\n",
        "    \"\"\"\n",
        "    if max_token_limit is None:\n",
        "        max_token_limit = MAX_CONTEXT_LIMIT\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"system\", \"content\": agent_instructions}] + messages,\n",
        "        tokenize=True\n",
        "    )\n",
        "    base_tokens = len(inputs)\n",
        "\n",
        "    if base_tokens >= max_token_limit:\n",
        "        return \"[...omitted due to context budget...]\"\n",
        "\n",
        "    enc = tokenizer.encode(content)\n",
        "    if base_tokens + len(enc) > max_token_limit:\n",
        "        keep = max_token_limit - base_tokens\n",
        "        enc = enc[:max(0, keep)]\n",
        "        content = tokenizer.decode(enc, skip_special_tokens=True) + \"\\n\\n[...Content Truncated due to Context Limit...]\"\n",
        "    return content\n",
        "\n",
        "def execute_tool_call(tool_name: str, tools: Dict[str, Any], args: Dict[str, Any]) -> str:\n",
        "    if tool_name not in tools:\n",
        "        raise KeyError(f\"Tool not found: {tool_name}\")\n",
        "    return tools[tool_name](**args)\n",
        "\n",
        "def safe_json_loads(s: str) -> Any:\n",
        "    s = s.strip()\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        pass\n",
        "    s2 = re.sub(r\"^```(json)?\\s*|\\s*```$\", \"\", s, flags=re.MULTILINE).strip()\n",
        "    try:\n",
        "        return json.loads(s2)\n",
        "    except Exception:\n",
        "        pass\n",
        "    m = re.search(r\"(\\{.*\\}|\\[.*\\])\", s2, flags=re.DOTALL)\n",
        "    if not m:\n",
        "        raise ValueError(\"No JSON object found in text\")\n",
        "    return json.loads(m.group(1))\n",
        "\n",
        "def hash_key(*parts: str) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    for p in parts:\n",
        "        h.update(p.encode(\"utf-8\"))\n",
        "    return h.hexdigest()[:16]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nudMxLgaWdTm"
      },
      "source": [
        "## 2) Shared Utils (schema, tool runner, token budget)\n",
        "\n",
        "- Upstage Solar-Pro 계열은 컨텍스트가 대략 64K 수준이므로, 안전한 한계치로 60K를 사용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "l2GX7JAmWeY7"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "def function_to_schema(func) -> dict:\n",
        "    sig = inspect.signature(func)\n",
        "    props = {}\n",
        "    required = []\n",
        "    for name, param in sig.parameters.items():\n",
        "        if name in (\"self\",):\n",
        "            continue\n",
        "        ann = param.annotation\n",
        "        jtype = \"string\"\n",
        "        if ann in (int,):\n",
        "            jtype = \"integer\"\n",
        "        elif ann in (float,):\n",
        "            jtype = \"number\"\n",
        "        elif ann in (bool,):\n",
        "            jtype = \"boolean\"\n",
        "        elif ann in (list, List):\n",
        "            jtype = \"array\"\n",
        "        elif ann in (dict, Dict):\n",
        "            jtype = \"object\"\n",
        "        props[name] = {\"type\": jtype}\n",
        "        if param.default is inspect._empty:\n",
        "            required.append(name)\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": func.__name__,\n",
        "            \"description\": (func.__doc__ or \"\").strip(),\n",
        "            \"parameters\": {\"type\": \"object\", \"properties\": props, \"required\": required}\n",
        "        }\n",
        "    }\n",
        "\n",
        "def truncate_tokens_if_needed(tokenizer, agent_instructions, messages, content, max_token_limit=None):\n",
        "    \"\"\"\n",
        "    - base가 이미 limit을 넘으면(히스토리 과다) 에러 내지 말고 내용을 최소화해서 계속 진행\n",
        "    - content가 넘치면 content만 잘라서 limit 안으로 넣기\n",
        "    \"\"\"\n",
        "    if max_token_limit is None:\n",
        "        max_token_limit = MAX_CONTEXT_LIMIT\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        [{\"role\": \"system\", \"content\": agent_instructions}] + messages,\n",
        "        tokenize=True\n",
        "    )\n",
        "    base_tokens = len(inputs)\n",
        "\n",
        "    if base_tokens >= max_token_limit:\n",
        "        return \"[...omitted due to context budget...]\"\n",
        "\n",
        "    enc = tokenizer.encode(content)\n",
        "    if base_tokens + len(enc) > max_token_limit:\n",
        "        keep = max_token_limit - base_tokens\n",
        "        enc = enc[:max(0, keep)]\n",
        "        content = tokenizer.decode(enc, skip_special_tokens=True) + \"\\n\\n[...Content Truncated due to Context Limit...]\"\n",
        "    return content\n",
        "\n",
        "def execute_tool_call(tool_name: str, tools: Dict[str, Any], args: Dict[str, Any]) -> str:\n",
        "    if tool_name not in tools:\n",
        "        raise KeyError(f\"Tool not found: {tool_name}\")\n",
        "    return tools[tool_name](**args)\n",
        "\n",
        "def safe_json_loads(s: str) -> Any:\n",
        "    s = s.strip()\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        pass\n",
        "    s2 = re.sub(r\"^```(json)?\\s*|\\s*```$\", \"\", s, flags=re.MULTILINE).strip()\n",
        "    try:\n",
        "        return json.loads(s2)\n",
        "    except Exception:\n",
        "        pass\n",
        "    m = re.search(r\"(\\{.*\\}|\\[.*\\])\", s2, flags=re.DOTALL)\n",
        "    if not m:\n",
        "        raise ValueError(\"No JSON object found in text\")\n",
        "    return json.loads(m.group(1))\n",
        "\n",
        "def hash_key(*parts: str) -> str:\n",
        "    h = hashlib.sha256()\n",
        "    for p in parts:\n",
        "        h.update(p.encode(\"utf-8\"))\n",
        "    return h.hexdigest()[:16]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj1K1AXEWha3"
      },
      "source": [
        "## 3) Core Tools\n",
        "\n",
        "필수 Tool 기능:\n",
        "1) 인터넷 검색(serper.dev)  \n",
        "2) 인터넷 파일 다운로드  \n",
        "3) PDF → Markdown 파싱(Upstage Document Parse)  \n",
        "4) Vector DB 저장/조회(Chroma + Upstage embedding)  \n",
        "5) LLM-as-Judge Rerank (Top-K 재정렬)\n",
        "\n",
        "> 한국 기업/시장 분석을 우선하기 위해 검색 쿼리에 한국 소스 힌트를 자동으로 섞습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "04ULjJikWh53"
      },
      "outputs": [],
      "source": [
        "class Agent(BaseModel):\n",
        "    name: str = \"Agent\"\n",
        "    model: str = \"solar-pro2-250909\"\n",
        "    instructions: str = \"You are a helpful agent.\"\n",
        "    tools: List[Any] = Field(default_factory=list)\n",
        "\n",
        "def run_agent(messages: List[Dict[str, Any]], agent: Agent, max_context_limit: int = None) -> str:\n",
        "    \"\"\"\n",
        "    OpenAI tool-calling 스타일 루프 실행.\n",
        "    - tools가 없으면 tools/tool_choice 파라미터를 아예 보내지 않는다(Upstage 400 방지)\n",
        "    \"\"\"\n",
        "    if max_context_limit is None:\n",
        "        max_context_limit = MAX_CONTEXT_LIMIT\n",
        "\n",
        "    tool_schemas = [function_to_schema(t) for t in agent.tools]\n",
        "    tool_map = {t.__name__: t for t in agent.tools}\n",
        "\n",
        "    while True:\n",
        "        kwargs = dict(\n",
        "            model=agent.model,\n",
        "            messages=[{\"role\":\"system\",\"content\": agent.instructions}] + messages,\n",
        "        )\n",
        "        if tool_schemas:\n",
        "            kwargs[\"tools\"] = tool_schemas\n",
        "            kwargs[\"tool_choice\"] = \"auto\"\n",
        "\n",
        "        resp = client.chat.completions.create(**kwargs)\n",
        "        msg = resp.choices[0].message\n",
        "\n",
        "        if not getattr(msg, \"tool_calls\", None):\n",
        "            content = msg.content or \"\"\n",
        "            content = truncate_tokens_if_needed(tokenizer, agent.instructions, messages, content, max_token_limit=max_context_limit)\n",
        "            return content\n",
        "\n",
        "        for tc in msg.tool_calls:\n",
        "            tool_name = tc.function.name\n",
        "            args = json.loads(tc.function.arguments or \"{}\")\n",
        "            try:\n",
        "                out = execute_tool_call(tool_name, tool_map, args)\n",
        "                if not isinstance(out, str):\n",
        "                    out = json.dumps(out, ensure_ascii=False)\n",
        "            except Exception as e:\n",
        "                out = f\"ToolError: {e}\"\n",
        "\n",
        "            out = truncate_tokens_if_needed(tokenizer, agent.instructions, messages, out, max_token_limit=max_context_limit)\n",
        "\n",
        "            messages.append({\"role\":\"assistant\",\"content\": None, \"tool_calls\":[tc]})\n",
        "            messages.append({\"role\":\"tool\",\"tool_call_id\": tc.id, \"content\": out})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fHejnUzgb7xX"
      },
      "outputs": [],
      "source": [
        "class RequestModel(BaseModel):\n",
        "    raw_request: str\n",
        "    language: str = \"ko\"\n",
        "    tone: str = \"concise\"\n",
        "    mode: Literal[\"fast\",\"standard\",\"deep\"] = \"standard\"\n",
        "\n",
        "class IdeaSchemaModel(BaseModel):\n",
        "    problem: str\n",
        "    target: str\n",
        "    solution: str\n",
        "    differentiation: str\n",
        "    business_model: str\n",
        "    industry: str\n",
        "    keywords: List[str] = Field(default_factory=list)\n",
        "    persona_hypotheses: List[str] = Field(default_factory=list)\n",
        "\n",
        "class EvidenceQueryPlan(BaseModel):\n",
        "    question: str\n",
        "    queries: List[str]\n",
        "    preferred_sources: List[str] = Field(default_factory=list)\n",
        "\n",
        "class EvidenceItem(BaseModel):\n",
        "    source_url: str\n",
        "    title: str = \"\"\n",
        "    snippet: str = \"\"\n",
        "    local_path: Optional[str] = None\n",
        "    parsed_markdown: Optional[str] = None\n",
        "\n",
        "class EvidenceStoreModel(BaseModel):\n",
        "    collection: str\n",
        "    items: List[EvidenceItem] = Field(default_factory=list)\n",
        "    version: str = \"v1\"\n",
        "\n",
        "class SignalsModel(BaseModel):\n",
        "    market: str\n",
        "    competition: str\n",
        "    customer: str\n",
        "    risks: str\n",
        "    score_explainable: Dict[str, float]\n",
        "\n",
        "class VerdictModel(BaseModel):\n",
        "    decision: Literal[\"GO\",\"NO_GO\",\"PIVOT\"]\n",
        "    key_reasons: List[str]\n",
        "    evidence_links: List[str]\n",
        "    next_actions: List[str]\n",
        "\n",
        "class ArtifactsModel(BaseModel):\n",
        "    prd_1p: str\n",
        "    scope_must_should_could: str\n",
        "    erd_mermaid: str\n",
        "    user_flow: str\n",
        "    roadmap_2_4_weeks: str\n",
        "    validation_plan: str\n",
        "\n",
        "class GuardsModel(BaseModel):\n",
        "    policy_violation: bool = False\n",
        "    token_overflow: bool = False\n",
        "    copyright_risk: bool = False\n",
        "    evidence_insufficient: bool = False\n",
        "    notes: List[str] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "class MarketModelModel(BaseModel):\n",
        "    # C 파트(거시→미시→생태계) 요약\n",
        "    market_scope: str\n",
        "    tam_sam_som: Dict[str, Any] = Field(default_factory=dict)\n",
        "    cagr: Dict[str, Any] = Field(default_factory=dict)\n",
        "    ecosystem: Dict[str, Any] = Field(default_factory=dict)\n",
        "    risks: List[str] = Field(default_factory=list)\n",
        "    evidence_links: List[str] = Field(default_factory=list)\n",
        "    confidence: Dict[str, float] = Field(default_factory=dict)\n",
        "\n",
        "class CostSimModel(BaseModel):\n",
        "    # B 파트(최소비용 개발계획 + 비용/일정 시뮬레이션)\n",
        "    mvp_scope: Dict[str, Any] = Field(default_factory=dict)\n",
        "    wbs: Dict[str, Any] = Field(default_factory=dict)\n",
        "    validation_plan: Dict[str, Any] = Field(default_factory=dict)\n",
        "    cost_model_assumptions: Dict[str, Any] = Field(default_factory=dict)\n",
        "    simulation: Dict[str, Any] = Field(default_factory=dict)\n",
        "    min_to_validate: Dict[str, Any] = Field(default_factory=dict)\n",
        "    assumptions_and_unknowns: List[Dict[str, Any]] = Field(default_factory=list)\n",
        "\n",
        "class WorkflowState(TypedDict, total=False):\n",
        "    request: Dict[str, Any]\n",
        "    idea_schema: Dict[str, Any]\n",
        "    evidence_plan: List[Dict[str, Any]]\n",
        "    evidence_store: Dict[str, Any]\n",
        "    evidence_pack: List[Dict[str, Any]]\n",
        "    market_plan: Dict[str, Any]\n",
        "    market_model: Dict[str, Any]\n",
        "    cost_sim: Dict[str, Any]\n",
        "    signals: Dict[str, Any]\n",
        "    verdict: Dict[str, Any]\n",
        "    artifacts: Dict[str, Any]\n",
        "    guards: Dict[str, Any]\n",
        "    final_report_markdown: str\n",
        "    intake: Dict[str, Any]\n",
        "    logs: List[Dict[str, Any]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Pp0BELB6brTK"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "import trafilatura\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "logging.getLogger(\"trafilatura\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"trafilatura.core\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"trafilatura.utils\").setLevel(logging.ERROR)\n",
        "\n",
        "def web_search(query: str, k: int = 10) -> List[Dict[str, str]]:\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "    payload = json.dumps({\"q\": query, \"num\": k})\n",
        "    headers = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
        "    r = requests.post(url, headers=headers, data=payload, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    out = []\n",
        "    for item in data.get(\"organic\", [])[:k]:\n",
        "        out.append({\"title\": item.get(\"title\",\"\"), \"link\": item.get(\"link\",\"\"), \"snippet\": item.get(\"snippet\",\"\")})\n",
        "    return out\n",
        "\n",
        "def is_pdf_url(url: str) -> bool:\n",
        "    return url.lower().split(\"?\")[0].endswith(\".pdf\")\n",
        "\n",
        "def fetch_url_text(url: str, timeout: int = 30, max_chars: int = 30000) -> str:\n",
        "    try:\n",
        "        r = requests.get(\n",
        "            url,\n",
        "            timeout=timeout,\n",
        "            headers={\"User-Agent\":\"Mozilla/5.0\"},\n",
        "            allow_redirects=True,\n",
        "        )\n",
        "        r.raise_for_status()\n",
        "\n",
        "        ctype = (r.headers.get(\"content-type\") or \"\").lower()\n",
        "        if (\"text/html\" not in ctype) and (\"application/xhtml\" not in ctype):\n",
        "            return \"\"\n",
        "\n",
        "        html = (r.text or \"\").strip()\n",
        "        if len(html) < 200:\n",
        "            return \"\"\n",
        "\n",
        "        text = trafilatura.extract(html, include_comments=False, include_tables=False, favor_recall=True) or \"\"\n",
        "        text = text.strip()\n",
        "        if len(text) > max_chars:\n",
        "            text = text[:max_chars]\n",
        "        return text\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def source_priority_score(url: str) -> int:\n",
        "    host = (urlparse(url).netloc or \"\").lower()\n",
        "    score = 0\n",
        "    if host.endswith(\".go.kr\"): score += 50\n",
        "    if host.endswith(\".ac.kr\"): score += 35\n",
        "    if host.endswith(\".or.kr\"): score += 25\n",
        "    if host.endswith(\".re.kr\"): score += 20\n",
        "    if host.endswith(\".kr\"): score += 10\n",
        "    if \"kosis\" in host or \"kostat\" in host: score += 50\n",
        "    if \"dart\" in host or \"fss\" in host: score += 40\n",
        "    if \"nipa\" in host or \"kisdi\" in host or \"kised\" in host: score += 25\n",
        "    return score\n",
        "\n",
        "def download_file(url: str, save_dir: str = \"./downloads\") -> str:\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    fn = re.sub(r\"[^a-zA-Z0-9_.-]\", \"_\", url.split(\"/\")[-1]) or f\"file_{int(time.time())}\"\n",
        "    path = os.path.join(save_dir, fn)\n",
        "    r = requests.get(url, timeout=120, headers={\"User-Agent\":\"Mozilla/5.0\"}, allow_redirects=True)\n",
        "    r.raise_for_status()\n",
        "    with open(path, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "    return path\n",
        "\n",
        "def parse_pdf_to_markdown(pdf_path: str) -> str:\n",
        "    url = \"https://api.upstage.ai/v1/document-ai/document-parse\"\n",
        "    headers = {\"Authorization\": f\"Bearer {UPSTAGE_API_KEY}\"}\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "        files = {\"document\": f}\n",
        "        data = {\n",
        "            \"model\": \"document-parse-250618\",\n",
        "            \"ocr\": \"auto\",\n",
        "            \"chart_recognition\": True,\n",
        "            \"coordinates\": True,\n",
        "            \"output_formats\": '[\"markdown\"]',\n",
        "            \"base64_encoding\": '[\"figure\"]',\n",
        "        }\n",
        "        r = requests.post(url, headers=headers, files=files, data=data, timeout=180)\n",
        "        r.raise_for_status()\n",
        "        j = r.json()\n",
        "    return j.get(\"content\", {}).get(\"markdown\", \"\")\n",
        "\n",
        "def _normalize_for_embedding(x: Any, max_chars: int = 8000) -> str:\n",
        "    s = (x if isinstance(x, str) else str(x) if x is not None else \"\").replace(\"\\x00\", \"\").strip()\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    if len(s) > max_chars:\n",
        "        s = s[:max_chars]\n",
        "    return s\n",
        "\n",
        "class UpstageEmbeddingFunction(EmbeddingFunction):\n",
        "    def __init__(\n",
        "        self,\n",
        "        client: OpenAI,\n",
        "        model: str = \"solar-embedding-1-large-query\",\n",
        "        batch_size: int = 16,\n",
        "        max_chars: int = 8000,\n",
        "        retries: int = 1,\n",
        "        backoff_sec: float = 1.0,\n",
        "    ):\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.batch_size = batch_size\n",
        "        self.max_chars = max_chars\n",
        "        self.retries = retries\n",
        "        self.backoff_sec = backoff_sec\n",
        "\n",
        "    def _embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
        "        last_err = None\n",
        "        for attempt in range(self.retries + 1):\n",
        "            try:\n",
        "                resp = self.client.embeddings.create(model=self.model, input=texts)\n",
        "                return [d.embedding for d in resp.data]\n",
        "            except Exception as e:\n",
        "                last_err = e\n",
        "                if attempt < self.retries:\n",
        "                    time.sleep(self.backoff_sec * (attempt + 1))\n",
        "        raise last_err\n",
        "\n",
        "    def _embed_one(self, text: str) -> List[float]:\n",
        "        resp = self.client.embeddings.create(model=self.model, input=text)\n",
        "        return resp.data[0].embedding\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if isinstance(input, str):\n",
        "            t = _normalize_for_embedding(input, max_chars=self.max_chars)\n",
        "            return [self._embed_one(t)] if t else [self._embed_one(\".\")]\n",
        "\n",
        "        raw = [_normalize_for_embedding(t, max_chars=self.max_chars) for t in list(input)]\n",
        "        texts = [t if t else \".\" for t in raw]\n",
        "\n",
        "        out: List[List[float]] = []\n",
        "        i = 0\n",
        "        while i < len(texts):\n",
        "            batch = texts[i:i+self.batch_size]\n",
        "            try:\n",
        "                out.extend(self._embed_batch(batch))\n",
        "            except Exception:\n",
        "                for t in batch:\n",
        "                    out.append(self._embed_one(t if t else \".\"))\n",
        "            i += self.batch_size\n",
        "        return out\n",
        "\n",
        "embedding_fn = UpstageEmbeddingFunction(client)\n",
        "\n",
        "def get_collection(name: str):\n",
        "    return chroma_client.get_or_create_collection(name=name, embedding_function=embedding_fn)\n",
        "\n",
        "def vectordb_upsert(collection: str, docs: List[str], metadatas: List[Dict[str, Any]], ids: List[str]) -> int:\n",
        "    clean_docs, clean_metas, clean_ids = [], [], []\n",
        "    for d, m, i in zip(docs, metadatas, ids):\n",
        "        s = _normalize_for_embedding(d, max_chars=8000)\n",
        "        if not s:\n",
        "            continue\n",
        "        clean_docs.append(s)\n",
        "        clean_metas.append(m)\n",
        "        clean_ids.append(i)\n",
        "\n",
        "    if not clean_ids:\n",
        "        return 0\n",
        "\n",
        "    col = get_collection(collection)\n",
        "    col.upsert(documents=clean_docs, metadatas=clean_metas, ids=clean_ids)\n",
        "    return len(clean_ids)\n",
        "\n",
        "def vectordb_query(collection: str, query: str, n_results: int = 8) -> Dict[str, Any]:\n",
        "    col = get_collection(collection)\n",
        "    return col.query(query_texts=[query], n_results=n_results)\n",
        "\n",
        "def _extract_rerank_items(data: Any) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Robustly extract rerank score items from various LLM JSON shapes.\n",
        "\n",
        "    Acceptable shapes seen in the wild:\n",
        "    - {\"scores\": [ {i, score, reason}, ... ]}\n",
        "    - [{\"i\":..., \"score\":...}, ...]\n",
        "    - [{\"results\": [...] }] / {\"results\":[...]}\n",
        "    - {\"ranking\": {\"1\": {...}, \"2\": {...}, ...}} or {\"ranking\":[...]}\n",
        "    - Nested wrappers containing any of the above\n",
        "    \"\"\"\n",
        "    def _as_list(x: Any) -> List[Any]:\n",
        "        return x if isinstance(x, list) else []\n",
        "\n",
        "    # If it's already a list, it might be either the items OR a wrapper list containing a dict.\n",
        "    if isinstance(data, list):\n",
        "        flattened: List[Dict[str, Any]] = []\n",
        "        for el in data:\n",
        "            flattened.extend(_extract_rerank_items(el))\n",
        "        # If we managed to extract anything from elements, use that.\n",
        "        if flattened:\n",
        "            return flattened\n",
        "        # Otherwise assume it's already the item list.\n",
        "        return [el for el in data if isinstance(el, dict)]\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        # direct hits\n",
        "        if isinstance(data.get(\"scores\"), list):\n",
        "            return [el for el in data[\"scores\"] if isinstance(el, dict)]\n",
        "        if isinstance(data.get(\"results\"), list):\n",
        "            return [el for el in data[\"results\"] if isinstance(el, dict)]\n",
        "        if \"ranking\" in data:\n",
        "            r = data[\"ranking\"]\n",
        "            if isinstance(r, list):\n",
        "                return [el for el in r if isinstance(el, dict)]\n",
        "            if isinstance(r, dict):\n",
        "                # Often ranking keys are \"1\",\"2\",...\n",
        "                try:\n",
        "                    keys = sorted(r.keys(), key=lambda k: int(str(k)))\n",
        "                except Exception:\n",
        "                    keys = list(r.keys())\n",
        "                return [r[k] for k in keys if isinstance(r.get(k), dict)]\n",
        "\n",
        "        # recursive search through nested values (first match wins)\n",
        "        for v in data.values():\n",
        "            items = _extract_rerank_items(v)\n",
        "            if items:\n",
        "                return items\n",
        "\n",
        "    return []\n",
        "\n",
        "def llm_rerank(query: str, candidates: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "    packed = [{\"i\": i, \"text\": (c.get(\"text\",\"\")[:1200]), \"meta\": c.get(\"meta\",{})} for i,c in enumerate(candidates)]\n",
        "    prompt = {\n",
        "        \"task\": \"rerank\",\n",
        "        \"schema\": {\"scores\": [{\"i\": 0, \"score\": 0.0, \"reason\": \"string\"}]},\n",
        "        \"instruction\": (\n",
        "            \"Score EACH candidate 0-10 by relevance to the query. \"\n",
        "            \"Prefer authoritative Korea-specific sources. \"\n",
        "            \"Return JSON ONLY exactly in this schema: \"\n",
        "            \"{\\\"scores\\\": [{\\\"i\\\": 0, \\\"score\\\": 8.5, \\\"reason\\\": \\\"...\\\"}, ...]}\"\n",
        "        ),\n",
        "        \"query\": query,\n",
        "        \"candidates\": packed\n",
        "    }\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=\"solar-pro2-250909\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Return JSON only. No prose. No markdown.\"},\n",
        "            {\"role\": \"user\", \"content\": json.dumps(prompt, ensure_ascii=False)}\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    raw = resp.choices[0].message.content\n",
        "    data = safe_json_loads(raw)\n",
        "\n",
        "    scored_items = _extract_rerank_items(data)\n",
        "\n",
        "    norm: List[Tuple[int, float, str]] = []\n",
        "    for item in scored_items:\n",
        "        if not isinstance(item, dict):\n",
        "            print(f\"Warning: Item in scored list is not a dictionary. Skipping: {item}\")\n",
        "            continue\n",
        "\n",
        "        idx = item.get(\"i\", item.get(\"index\", item.get(\"id\", item.get(\"idx\"))))\n",
        "        if idx is None:\n",
        "            print(f\"Warning: Item in scored list missing 'i' key: {item}. Skipping.\")\n",
        "            continue\n",
        "        try:\n",
        "            idx = int(idx)\n",
        "        except Exception:\n",
        "            print(f\"Warning: Item index 'i' is not an integer: {item}. Skipping.\")\n",
        "            continue\n",
        "        if idx < 0 or idx >= len(candidates):\n",
        "            print(f\"Warning: Item index {idx} out of bounds (candidates len {len(candidates)}): {item}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        score = item.get(\"score\", item.get(\"sco\", item.get(\"s\", item.get(\"relevance\", 0))))\n",
        "        try:\n",
        "            score_f = float(score)\n",
        "        except Exception:\n",
        "            print(f\"Warning: Item score is not a float: {item}. Defaulting to 0.0.\")\n",
        "            score_f = 0.0\n",
        "\n",
        "        reason = item.get(\"reason\") or item.get(\"rationale\") or item.get(\"why\") or \"\"\n",
        "        norm.append((idx, score_f, str(reason)))\n",
        "\n",
        "    # Keep best score per candidate index\n",
        "    best: Dict[int, Tuple[float, str]] = {}\n",
        "    for idx, score_f, reason in norm:\n",
        "        if (idx not in best) or (score_f > best[idx][0]):\n",
        "            best[idx] = (score_f, reason)\n",
        "\n",
        "    if not best:\n",
        "        # Fallback: prioritize by source domain quality if rerank output is unusable\n",
        "        merged = []\n",
        "        for c in candidates:\n",
        "            meta = c.get(\"meta\", {}) or {}\n",
        "            url = meta.get(\"url\") or meta.get(\"link\") or \"\"\n",
        "            merged.append({**c, \"score\": float(source_priority_score(url)), \"reason\": \"fallback: source_priority_score\"})\n",
        "        merged.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "        return merged[:top_k]\n",
        "\n",
        "    merged = [{**candidates[i], \"score\": best[i][0], \"reason\": best[i][1]} for i in best]\n",
        "    merged.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    return merged[:top_k]\n",
        "\n",
        "def simple_chunk(text: str, max_chars: int = 1500, overlap: int = 200) -> List[str]:\n",
        "    text = (text or \"\").strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    step = max(1, max_chars - overlap)\n",
        "    while i < len(text):\n",
        "        c = text[i:i+max_chars].strip()\n",
        "        if c:\n",
        "            chunks.append(c)\n",
        "        i += step\n",
        "    return chunks\n",
        "EVIDENCE_PLAN_PROMPT = '''\n",
        "역할: 아이디어를 검증하기 위한 '질문 리스트'와 각 질문별 '검색 쿼리'를 만든다.\n",
        "출력(JSON only):\n",
        "[\n",
        "  {\"question\":\"...\", \"queries\":[\"...\",\"...\",\"...\"], \"preferred_sources\":[\"gov\",\"kosis\",\"dart\",\"research\"]},\n",
        "  ...\n",
        "]\n",
        "규칙:\n",
        "- 한국 시장/한국 기업 중심 쿼리로 작성\n",
        "- 각 question당 queries는 3개 이내\n",
        "- question은 4~6개\n",
        "'''\n",
        "evidence_plan_agent = Agent(\n",
        "    name=\"EvidencePlanMaker\",\n",
        "    instructions=EVIDENCE_PLAN_PROMPT,\n",
        "    tools=[],\n",
        ")\n",
        "\n",
        "def make_evidence_plan(idea_schema: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    messages = [{\"role\":\"user\",\"content\": json.dumps({\"idea_schema\": idea_schema}, ensure_ascii=False)}]\n",
        "    out = run_agent(messages, evidence_plan_agent)\n",
        "    plan = safe_json_loads(out)\n",
        "    if not isinstance(plan, list):\n",
        "        raise ValueError(\"evidence_plan must be a list\")\n",
        "    return plan\n",
        "\n",
        "def check_vectordb_cache(collection: str) -> bool:\n",
        "    try:\n",
        "        chroma_client.get_collection(name=collection)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def build_expanded_queries(q: str) -> List[str]:\n",
        "    years = [\"2025\", \"2024\", \"2023\"]\n",
        "    tails = [\"시장 규모\", \"시장 동향 보고서\", \"통계\", \"백서\", \"TAM SAM SOM\", \"경쟁사\", \"규제\", \"지원사업\"]\n",
        "    out = []\n",
        "    for y in years:\n",
        "        out.append(f\"{q} {y}\")\n",
        "    for t in tails:\n",
        "        out.append(f\"{q} {t}\")\n",
        "    out += [f\"{q} site:go.kr\", f\"{q} site:kosis.kr\", f\"{q} DART 공시\"]\n",
        "    return list(dict.fromkeys(out))\n",
        "\n",
        "def evidence_builder_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    idea = state[\"idea_schema\"]\n",
        "    collection = f\"ideaproof_{hash_key(json.dumps(idea, ensure_ascii=False))}\"\n",
        "\n",
        "    mode = (state.get(\"request\", {}) or {}).get(\"mode\", \"standard\")\n",
        "    if mode == \"fast\":\n",
        "        MAX_RESULTS_PER_QUERY_1 = 5\n",
        "        MAX_SOURCES_1 = 12\n",
        "        MAX_CHUNKS_PER_SOURCE = 6\n",
        "        MAX_TOTAL_CHUNKS = 120\n",
        "        MIN_STORED_CHUNKS = 50\n",
        "        DO_EXPAND = False\n",
        "        DO_PDF_PARSE = False\n",
        "        TIME_BUDGET_SEC = 6 * 60\n",
        "    elif mode == \"deep\":\n",
        "        MAX_RESULTS_PER_QUERY_1 = 10\n",
        "        MAX_SOURCES_1 = 45\n",
        "        MAX_CHUNKS_PER_SOURCE = 14\n",
        "        MAX_TOTAL_CHUNKS = 450\n",
        "        MIN_STORED_CHUNKS = 180\n",
        "        DO_EXPAND = True\n",
        "        DO_PDF_PARSE = True\n",
        "        TIME_BUDGET_SEC = 18 * 60\n",
        "    else:\n",
        "        MAX_RESULTS_PER_QUERY_1 = 8\n",
        "        MAX_SOURCES_1 = 25\n",
        "        MAX_CHUNKS_PER_SOURCE = 10\n",
        "        MAX_TOTAL_CHUNKS = 260\n",
        "        MIN_STORED_CHUNKS = 120\n",
        "        DO_EXPAND = True\n",
        "        DO_PDF_PARSE = True\n",
        "        TIME_BUDGET_SEC = 12 * 60\n",
        "\n",
        "    def time_left() -> float:\n",
        "        return TIME_BUDGET_SEC - (time.time() - t0)\n",
        "\n",
        "    if check_vectordb_cache(collection):\n",
        "        state[\"evidence_store\"] = EvidenceStoreModel(collection=collection, items=[], version=\"v3\").model_dump()\n",
        "        logs.append({\"node\":\"evidence_builder\", \"t\": time.time()-t0, \"cache\":\"HIT\", \"collection\": collection, \"mode\": mode})\n",
        "        state[\"logs\"] = logs\n",
        "        return state\n",
        "\n",
        "    plan = make_evidence_plan(idea)\n",
        "    state[\"evidence_plan\"] = plan\n",
        "\n",
        "    def run_harvest(queries: List[str], max_results_per_query: int, max_sources_total: int):\n",
        "        results = []\n",
        "        for q in queries:\n",
        "            if time_left() <= 0:\n",
        "                break\n",
        "            try:\n",
        "                for r in web_search(q, k=max_results_per_query):\n",
        "                    if r.get(\"link\"):\n",
        "                        results.append(r)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        uniq = {}\n",
        "        for r in results:\n",
        "            uniq[r[\"link\"]] = r\n",
        "        ranked = list(uniq.values())\n",
        "        ranked.sort(key=lambda x: source_priority_score(x[\"link\"]), reverse=True)\n",
        "        ranked = ranked[:max_sources_total]\n",
        "\n",
        "        items: List[EvidenceItem] = []\n",
        "        all_chunks, all_metas, all_ids = [], [], []\n",
        "\n",
        "        for r in ranked:\n",
        "            if time_left() <= 0 or len(all_chunks) >= MAX_TOTAL_CHUNKS:\n",
        "                break\n",
        "\n",
        "            url = r[\"link\"]\n",
        "            title = r.get(\"title\",\"\")\n",
        "            snippet = r.get(\"snippet\",\"\")\n",
        "\n",
        "            text = \"\"\n",
        "            local = None\n",
        "            md_text = None\n",
        "\n",
        "            if is_pdf_url(url):\n",
        "                if DO_PDF_PARSE and source_priority_score(url) >= 30 and time_left() > 60:\n",
        "                    try:\n",
        "                        local = download_file(url)\n",
        "                        md_text = parse_pdf_to_markdown(local)\n",
        "                        text = (md_text or \"\").strip()\n",
        "                    except Exception:\n",
        "                        text = \"\"\n",
        "                else:\n",
        "                    text = \"\"\n",
        "            else:\n",
        "                text = fetch_url_text(url)\n",
        "\n",
        "            if not text:\n",
        "                text = f\"{title}\\n{snippet}\\nURL: {url}\"\n",
        "\n",
        "            items.append(EvidenceItem(source_url=url, title=title, snippet=snippet, local_path=local, parsed_markdown=md_text))\n",
        "\n",
        "            chunks = simple_chunk(text, max_chars=1500, overlap=200)[:MAX_CHUNKS_PER_SOURCE]\n",
        "            for j, ch in enumerate(chunks):\n",
        "                if len(all_chunks) >= MAX_TOTAL_CHUNKS:\n",
        "                    break\n",
        "                cid = hash_key(collection, url, str(j))\n",
        "                all_chunks.append(ch)\n",
        "                all_metas.append({\"url\": url, \"title\": title, \"chunk\": j})\n",
        "                all_ids.append(cid)\n",
        "\n",
        "        stored = 0\n",
        "        if all_chunks:\n",
        "            stored = vectordb_upsert(collection, all_chunks, all_metas, all_ids)\n",
        "        return items, stored\n",
        "\n",
        "    base_queries = []\n",
        "    for p in plan:\n",
        "        for q in (p.get(\"queries\") or [])[:3]:\n",
        "            base_queries.append(q)\n",
        "    base_queries = list(dict.fromkeys(base_queries))[:15]\n",
        "\n",
        "    items, stored = run_harvest(base_queries, max_results_per_query=MAX_RESULTS_PER_QUERY_1, max_sources_total=MAX_SOURCES_1)\n",
        "\n",
        "    if DO_EXPAND and (stored < MIN_STORED_CHUNKS) and (time_left() > 90):\n",
        "        expanded = []\n",
        "        for p in plan:\n",
        "            expanded += build_expanded_queries(p.get(\"question\",\"\"))\n",
        "        expanded = list(dict.fromkeys(expanded))[:20]\n",
        "\n",
        "        items2, stored2 = run_harvest(expanded, max_results_per_query=MAX_RESULTS_PER_QUERY_1, max_sources_total=max(MAX_SOURCES_1, 30))\n",
        "        merged = {it.source_url: it for it in (items + items2)}\n",
        "        items = list(merged.values())\n",
        "        stored = max(stored, stored2)\n",
        "\n",
        "    state[\"evidence_store\"] = EvidenceStoreModel(collection=collection, items=items, version=\"v3\").model_dump()\n",
        "\n",
        "    logs.append({\n",
        "        \"node\":\"evidence_builder\",\n",
        "        \"t\": time.time()-t0,\n",
        "        \"cache\":\"MISS\",\n",
        "        \"collection\": collection,\n",
        "        \"mode\": mode,\n",
        "        \"sources\": len(items),\n",
        "        \"stored_chunks\": stored,\n",
        "        \"time_budget_sec\": TIME_BUDGET_SEC,\n",
        "        \"time_budget_hit\": (time_left() <= 0)\n",
        "    })\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-R0KKMnjcC3-"
      },
      "outputs": [],
      "source": [
        "INTAKE_PROMPT = \"\"\"\n",
        "역할: 요청을 '아이디어 검증/설계 워크플로'로 처리할지, 일상대화로 처리할지 라우팅한다.\n",
        "목표:\n",
        "1) request 정규화(언어/톤/모드)\n",
        "2) 아이디어 입력이 부족하면 '최소 질문'으로 보완 질문을 만든다.\n",
        "출력(JSON only):\n",
        "{\n",
        "  \"route\": \"workflow\" | \"chat\",\n",
        "  \"request\": {\"raw_request\": \"...\", \"language\": \"ko\", \"tone\": \"concise\", \"mode\":\"fast|standard|deep\"},\n",
        "  \"missing_fields\": [\"problem\",\"target\",\"solution\",\"differentiation\",\"business_model\"],\n",
        "  \"clarifying_questions\": [\"...\",\"...\"]\n",
        "}\n",
        "규칙:\n",
        "- 질문은 최대 5개. 선택형/단답형 우선.\n",
        "\"\"\"\n",
        "\n",
        "intake_agent = Agent(name=\"IntakeRouteClarify\", instructions=INTAKE_PROMPT, tools=[])\n",
        "\n",
        "def intake_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    raw = state[\"request\"][\"raw_request\"]\n",
        "    messages = [{\"role\":\"user\",\"content\": raw}]\n",
        "    out = run_agent(messages, intake_agent)\n",
        "    data = safe_json_loads(out)\n",
        "\n",
        "    state[\"request\"] = data[\"request\"]\n",
        "    state[\"intake\"] = data\n",
        "\n",
        "    guards = state.get(\"guards\", {})\n",
        "    guards.setdefault(\"notes\", [])\n",
        "    guards[\"notes\"].append(f\"route={data.get('route')}\")\n",
        "    state[\"guards\"] = guards\n",
        "\n",
        "    logs.append({\"node\":\"intake\", \"t\": time.time()-t0, \"route\": data.get(\"route\")})\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n",
        "\n",
        "def route_after_intake(state: WorkflowState) -> str:\n",
        "    route = state.get(\"intake\", {}).get(\"route\", \"workflow\")\n",
        "    return \"chat_end\" if route == \"chat\" else \"structurer\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Tc6hzm7FcD3i"
      },
      "outputs": [],
      "source": [
        "STRUCTURER_PROMPT = \"\"\"\n",
        "역할: 아이디어를 문제/대상/해결/차별/BM로 구조화하고, 산업 분류 및 키워드를 만든다.\n",
        "입력:\n",
        "- raw idea text(자유형)\n",
        "출력(JSON only):\n",
        "{\n",
        "  \"problem\": \"...\",\n",
        "  \"target\": \"...\",\n",
        "  \"solution\": \"...\",\n",
        "  \"differentiation\": \"...\",\n",
        "  \"business_model\": \"...\",\n",
        "  \"industry\": \"...\",\n",
        "  \"keywords\": [\"...\"],\n",
        "  \"persona_hypotheses\": [\"...\"]\n",
        "}\n",
        "규칙:\n",
        "- 모호하면 가능한 가설을 1~2개로 제한해 persona_hypotheses에 넣고, 단정하지 말 것.\n",
        "\"\"\"\n",
        "\n",
        "structurer_agent = Agent(name=\"Structurer\", instructions=STRUCTURER_PROMPT, tools=[])\n",
        "\n",
        "def structurer_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    raw = state[\"request\"][\"raw_request\"]\n",
        "    messages = [{\"role\":\"user\",\"content\": raw}]\n",
        "    out = run_agent(messages, structurer_agent)\n",
        "    data = safe_json_loads(out)\n",
        "\n",
        "    obj = IdeaSchemaModel(**data)\n",
        "    state[\"idea_schema\"] = obj.model_dump()\n",
        "\n",
        "    logs.append({\"node\":\"structurer\", \"t\": time.time()-t0, \"industry\": obj.industry})\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "CJXMOWnwcFrf"
      },
      "outputs": [],
      "source": [
        "EVIDENCE_PLAN_PROMPT = '''\n",
        "역할: 아이디어를 검증하기 위한 '질문 리스트'와 각 질문별 '검색 쿼리'를 만든다.\n",
        "출력(JSON only):\n",
        "[\n",
        "  {\"question\":\"...\", \"queries\":[\"...\",\"...\",\"...\"], \"preferred_sources\":[\"gov\",\"kosis\",\"dart\",\"research\"]},\n",
        "  ...\n",
        "]\n",
        "규칙:\n",
        "- 한국 시장/한국 기업 중심 쿼리로 작성\n",
        "- 각 question당 queries는 3개 이내\n",
        "- question은 4~6개\n",
        "'''\n",
        "evidence_plan_agent = Agent(\n",
        "    name=\"EvidencePlanMaker\",\n",
        "    instructions=EVIDENCE_PLAN_PROMPT,\n",
        "    tools=[],\n",
        ")\n",
        "\n",
        "def make_evidence_plan(idea_schema: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
        "    messages = [{\"role\":\"user\",\"content\": json.dumps({\"idea_schema\": idea_schema}, ensure_ascii=False)}]\n",
        "    out = run_agent(messages, evidence_plan_agent)\n",
        "    plan = safe_json_loads(out)\n",
        "    if not isinstance(plan, list):\n",
        "        raise ValueError(\"evidence_plan must be a list\")\n",
        "    return plan\n",
        "\n",
        "\n",
        "MARKET_PLAN_PROMPT_KR_V1 = \"\"\"역할: 한국(대한민국) 창업 아이디어를 검증하기 위한 '거시→자산군→업종/세그먼트→생태계' 리서치 플랜을 만든다.\n",
        "출력(JSON only):\n",
        "{\n",
        "  \"market_definition\": {\n",
        "    \"one_line_definition\": \"...\",\n",
        "    \"industry_labels\": {\"ksic_candidates\":[\"...\"], \"industry_keywords_kr\":[\"...\"]},\n",
        "    \"segment_definition\": {\"who\":\"...\", \"where\":\"대한민국\", \"use_case\":\"...\", \"pricing_anchor\":\"...\"},\n",
        "    \"value_chain_position\": \"...\"\n",
        "  },\n",
        "  \"layers\": [\n",
        "    {\n",
        "      \"layer_name\": \"macro|asset_class|industry_segment|ecosystem_firms\",\n",
        "      \"goals\": [\"...\"],\n",
        "      \"metric_targets\": [{\"name\":\"...\", \"why_it_matters\":\"...\", \"preferred_source_hint\":\"...\"}],\n",
        "      \"query_sets\": [{\"perspective\":\"시장|규제|고객|기술|재무|리스크|경쟁\", \"preferred_sources\":[\"...\"], \"queries\":[\"...\",\"...\"]}],\n",
        "      \"min_evidence\": 3,\n",
        "      \"caps\": {\"max_queries\":12, \"max_sources\":20, \"max_per_domain\":2},\n",
        "      \"success_criteria\": [\"...\"],\n",
        "      \"fallback\": {\"if_metrics_missing\":[\"...\"], \"notes\":\"...\"}\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "규칙:\n",
        "- 범위는 '대한민국'으로 고정. 해외/글로벌 언급 금지.\n",
        "- 숫자는 '원하는 지표'로만 제시(근거 없으면 생성 금지). 대신 success_criteria에 '정량 2개 이상 확보' 같은 기준을 둬라.\n",
        "- caps는 반드시 지켜라.\n",
        "- ksic_candidates는 확신 없으면 후보 1~3개로만.\"\"\"\n",
        "\n",
        "market_plan_agent = Agent(\n",
        "    name=\"MarketPlanKR\",\n",
        "    instructions=MARKET_PLAN_PROMPT_KR_V1,\n",
        "    tools=[],\n",
        ")\n",
        "\n",
        "def make_market_plan(idea_schema: Dict[str, Any], raw_request: str = \"\") -> Dict[str, Any]:\n",
        "    payload = {\"idea_schema\": idea_schema, \"raw_request\": raw_request}\n",
        "    messages = [{\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}]\n",
        "    out = run_agent(messages, market_plan_agent)\n",
        "    plan = safe_json_loads(out)\n",
        "    if not isinstance(plan, dict):\n",
        "        raise ValueError(\"market_plan must be a dict\")\n",
        "    return plan\n",
        "\n",
        "def flatten_market_plan_to_evidence_plan(market_plan: Dict[str, Any], max_questions: int = 6) -> List[Dict[str, Any]]:\n",
        "    # market_plan.layers[*].query_sets 를 evidence_builder가 쓰는 (question, queries, preferred_sources) 리스트로 평탄화\n",
        "    layers = (market_plan or {}).get(\"layers\", []) or []\n",
        "    out: List[Dict[str, Any]] = []\n",
        "    for layer in layers:\n",
        "        layer_name = (layer.get(\"layer_name\",\"\") or \"\").strip() or \"layer\"\n",
        "        goals = layer.get(\"goals\", []) or []\n",
        "        goal = goals[0] if goals else \"핵심 근거 수집\"\n",
        "        query_sets = layer.get(\"query_sets\", []) or []\n",
        "        queries: List[str] = []\n",
        "        preferred_sources: List[str] = []\n",
        "        for qs in query_sets:\n",
        "            preferred_sources += (qs.get(\"preferred_sources\", []) or [])\n",
        "            for q in (qs.get(\"queries\", []) or []):\n",
        "                if isinstance(q, str) and q.strip():\n",
        "                    queries.append(q.strip())\n",
        "        # caps: question당 3개 이내\n",
        "        queries = list(dict.fromkeys(queries))[:3]\n",
        "        preferred_sources = list(dict.fromkeys([s for s in preferred_sources if isinstance(s, str) and s.strip()]))[:6]\n",
        "\n",
        "        out.append({\n",
        "            \"question\": f\"[{layer_name}] {goal}\",\n",
        "            \"queries\": queries if queries else [f\"{layer_name} 대한민국 시장 규모 통계\", f\"{layer_name} 규제 동향 대한민국\"],\n",
        "            \"preferred_sources\": preferred_sources,\n",
        "        })\n",
        "        if len(out) >= max_questions:\n",
        "            break\n",
        "\n",
        "    # 최소 4개는 유지(부족하면 기본 질문으로 보강)\n",
        "    while len(out) < 4:\n",
        "        out.append({\n",
        "            \"question\": \"[general] 대한민국 시장/경쟁/규제 정량 근거 보강\",\n",
        "            \"queries\": [\"대한민국 시장 규모 통계\", \"대한민국 경쟁사 리스트\", \"대한민국 관련 규제 요약\"],\n",
        "            \"preferred_sources\": [\"kosis\",\"dart\",\"gov\"],\n",
        "        })\n",
        "    return out[:max_questions]\n",
        "\n",
        "\n",
        "def check_vectordb_cache(collection: str) -> bool:\n",
        "    try:\n",
        "        chroma_client.get_collection(name=collection)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def build_expanded_queries(q: str) -> List[str]:\n",
        "    years = [\"2025\", \"2024\", \"2023\"]\n",
        "    tails = [\"시장 규모\", \"시장 동향 보고서\", \"통계\", \"백서\", \"TAM SAM SOM\", \"경쟁사\", \"규제\", \"지원사업\"]\n",
        "    out = []\n",
        "    for y in years:\n",
        "        out.append(f\"{q} {y}\")\n",
        "    for t in tails:\n",
        "        out.append(f\"{q} {t}\")\n",
        "    out += [f\"{q} site:go.kr\", f\"{q} site:kosis.kr\", f\"{q} DART 공시\"]\n",
        "    return list(dict.fromkeys(out))\n",
        "\n",
        "def market_plan_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    idea = state.get(\"idea_schema\", {}) or {}\n",
        "    raw = (state.get(\"request\", {}) or {}).get(\"raw_request\",\"\")\n",
        "\n",
        "    try:\n",
        "        mp = make_market_plan(idea_schema=idea, raw_request=raw)\n",
        "    except Exception as e:\n",
        "        mp = {\"error\": str(e), \"layers\": []}\n",
        "\n",
        "    state[\"market_plan\"] = mp\n",
        "\n",
        "    if not state.get(\"evidence_plan\"):\n",
        "        try:\n",
        "            state[\"evidence_plan\"] = flatten_market_plan_to_evidence_plan(mp, max_questions=6)\n",
        "        except Exception:\n",
        "            state[\"evidence_plan\"] = make_evidence_plan(idea) if idea else []\n",
        "\n",
        "    logs.append({\"node\":\"market_plan\", \"t\": time.time()-t0, \"layers\": len((mp or {}).get(\"layers\", []) or [])})\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n",
        "\n",
        "def evidence_builder_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    idea = state[\"idea_schema\"]\n",
        "    collection = f\"ideaproof_{hash_key(json.dumps(idea, ensure_ascii=False))}\"\n",
        "\n",
        "    mode = (state.get(\"request\", {}) or {}).get(\"mode\", \"standard\")\n",
        "    if mode == \"fast\":\n",
        "        MAX_RESULTS_PER_QUERY_1 = 5\n",
        "        MAX_SOURCES_1 = 12\n",
        "        MAX_CHUNKS_PER_SOURCE = 6\n",
        "        MAX_TOTAL_CHUNKS = 120\n",
        "        MIN_STORED_CHUNKS = 50\n",
        "        DO_EXPAND = False\n",
        "        DO_PDF_PARSE = False\n",
        "        TIME_BUDGET_SEC = 6 * 60\n",
        "    elif mode == \"deep\":\n",
        "        MAX_RESULTS_PER_QUERY_1 = 10\n",
        "        MAX_SOURCES_1 = 45\n",
        "        MAX_CHUNKS_PER_SOURCE = 14\n",
        "        MAX_TOTAL_CHUNKS = 450\n",
        "        MIN_STORED_CHUNKS = 180\n",
        "        DO_EXPAND = True\n",
        "        DO_PDF_PARSE = True\n",
        "        TIME_BUDGET_SEC = 18 * 60\n",
        "    else:\n",
        "        MAX_RESULTS_PER_QUERY_1 = 8\n",
        "        MAX_SOURCES_1 = 25\n",
        "        MAX_CHUNKS_PER_SOURCE = 10\n",
        "        MAX_TOTAL_CHUNKS = 260\n",
        "        MIN_STORED_CHUNKS = 120\n",
        "        DO_EXPAND = True\n",
        "        DO_PDF_PARSE = True\n",
        "        TIME_BUDGET_SEC = 12 * 60\n",
        "\n",
        "    def time_left() -> float:\n",
        "        return TIME_BUDGET_SEC - (time.time() - t0)\n",
        "\n",
        "    if check_vectordb_cache(collection):\n",
        "        state[\"evidence_store\"] = EvidenceStoreModel(collection=collection, items=[], version=\"v3\").model_dump()\n",
        "        logs.append({\"node\":\"evidence_builder\", \"t\": time.time()-t0, \"cache\":\"HIT\", \"collection\": collection, \"mode\": mode})\n",
        "        state[\"logs\"] = logs\n",
        "        return state\n",
        "\n",
        "    plan = state.get(\"evidence_plan\")\n",
        "    if not plan:\n",
        "        mp = state.get(\"market_plan\")\n",
        "        if not mp:\n",
        "            mp = make_market_plan(idea_schema=idea, raw_request=(state.get(\"request\", {}) or {}).get(\"raw_request\",\"\"))\n",
        "            state[\"market_plan\"] = mp\n",
        "        try:\n",
        "            plan = flatten_market_plan_to_evidence_plan(mp, max_questions=6)\n",
        "        except Exception:\n",
        "            plan = make_evidence_plan(idea)\n",
        "    state[\"evidence_plan\"] = plan\n",
        "\n",
        "    def run_harvest(queries: List[str], max_results_per_query: int, max_sources_total: int):\n",
        "        results = []\n",
        "        for q in queries:\n",
        "            if time_left() <= 0:\n",
        "                break\n",
        "            try:\n",
        "                for r in web_search(q, k=max_results_per_query):\n",
        "                    if r.get(\"link\"):\n",
        "                        results.append(r)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        uniq = {}\n",
        "        for r in results:\n",
        "            uniq[r[\"link\"]] = r\n",
        "        ranked = list(uniq.values())\n",
        "        ranked.sort(key=lambda x: source_priority_score(x[\"link\"]), reverse=True)\n",
        "        ranked = ranked[:max_sources_total]\n",
        "\n",
        "        items: List[EvidenceItem] = []\n",
        "        all_chunks, all_metas, all_ids = [], [], []\n",
        "\n",
        "        for r in ranked:\n",
        "            if time_left() <= 0 or len(all_chunks) >= MAX_TOTAL_CHUNKS:\n",
        "                break\n",
        "\n",
        "            url = r[\"link\"]\n",
        "            title = r.get(\"title\",\"\")\n",
        "            snippet = r.get(\"snippet\",\"\")\n",
        "\n",
        "            text = \"\"\n",
        "            local = None\n",
        "            md_text = None\n",
        "\n",
        "            if is_pdf_url(url):\n",
        "                if DO_PDF_PARSE and source_priority_score(url) >= 30 and time_left() > 60:\n",
        "                    try:\n",
        "                        local = download_file(url)\n",
        "                        md_text = parse_pdf_to_markdown(local)\n",
        "                        text = (md_text or \"\").strip()\n",
        "                    except Exception:\n",
        "                        text = \"\"\n",
        "                else:\n",
        "                    text = \"\"\n",
        "            else:\n",
        "                text = fetch_url_text(url)\n",
        "\n",
        "            if not text:\n",
        "                text = f\"{title}\\n{snippet}\\nURL: {url}\"\n",
        "\n",
        "            items.append(EvidenceItem(source_url=url, title=title, snippet=snippet, local_path=local, parsed_markdown=md_text))\n",
        "\n",
        "            chunks = simple_chunk(text, max_chars=1500, overlap=200)[:MAX_CHUNKS_PER_SOURCE]\n",
        "            for j, ch in enumerate(chunks):\n",
        "                if len(all_chunks) >= MAX_TOTAL_CHUNKS:\n",
        "                    break\n",
        "                cid = hash_key(collection, url, str(j))\n",
        "                all_chunks.append(ch)\n",
        "                all_metas.append({\"url\": url, \"title\": title, \"chunk\": j})\n",
        "                all_ids.append(cid)\n",
        "\n",
        "        stored = 0\n",
        "        if all_chunks:\n",
        "            stored = vectordb_upsert(collection, all_chunks, all_metas, all_ids)\n",
        "        return items, stored\n",
        "\n",
        "    base_queries = []\n",
        "    for p in plan:\n",
        "        for q in (p.get(\"queries\") or [])[:3]:\n",
        "            base_queries.append(q)\n",
        "    base_queries = list(dict.fromkeys(base_queries))[:15]\n",
        "\n",
        "    items, stored = run_harvest(base_queries, max_results_per_query=MAX_RESULTS_PER_QUERY_1, max_sources_total=MAX_SOURCES_1)\n",
        "\n",
        "    if DO_EXPAND and (stored < MIN_STORED_CHUNKS) and (time_left() > 90):\n",
        "        expanded = []\n",
        "        for p in plan:\n",
        "            expanded += build_expanded_queries(p.get(\"question\",\"\"))\n",
        "        expanded = list(dict.fromkeys(expanded))[:20]\n",
        "\n",
        "        items2, stored2 = run_harvest(expanded, max_results_per_query=MAX_RESULTS_PER_QUERY_1, max_sources_total=max(MAX_SOURCES_1, 30))\n",
        "        merged = {it.source_url: it for it in (items + items2)}\n",
        "        items = list(merged.values())\n",
        "        stored = max(stored, stored2)\n",
        "\n",
        "    state[\"evidence_store\"] = EvidenceStoreModel(collection=collection, items=items, version=\"v3\").model_dump()\n",
        "\n",
        "    logs.append({\n",
        "        \"node\":\"evidence_builder\",\n",
        "        \"t\": time.time()-t0,\n",
        "        \"cache\":\"MISS\",\n",
        "        \"collection\": collection,\n",
        "        \"mode\": mode,\n",
        "        \"sources\": len(items),\n",
        "        \"stored_chunks\": stored,\n",
        "        \"time_budget_sec\": TIME_BUDGET_SEC,\n",
        "        \"time_budget_hit\": (time_left() <= 0)\n",
        "    })\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dXFvZ8AdcIop"
      },
      "outputs": [],
      "source": [
        "def extractor_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    plan = state.get(\"evidence_plan\", [])\n",
        "    collection = state[\"evidence_store\"][\"collection\"]\n",
        "\n",
        "    evidence_pack = []\n",
        "    for p in plan[:6]:\n",
        "        q = p.get(\"question\",\"\")\n",
        "        if not q:\n",
        "            continue\n",
        "\n",
        "        raw = vectordb_query(collection=collection, query=q, n_results=12)\n",
        "        docs = raw.get(\"documents\", [[]])[0]\n",
        "        metas = raw.get(\"metadatas\", [[]])[0]\n",
        "\n",
        "        if not docs:\n",
        "            continue\n",
        "\n",
        "        candidates = [{\"text\": d, \"meta\": m} for d, m in zip(docs, metas)]\n",
        "        reranked = llm_rerank(query=q, candidates=candidates, top_k=5)\n",
        "\n",
        "        for r in reranked:\n",
        "            r[\"text\"] = (r.get(\"text\",\"\")[:800]).strip()\n",
        "\n",
        "        evidence_pack.append({\"question\": q, \"top_chunks\": reranked})\n",
        "\n",
        "    state[\"evidence_pack\"] = evidence_pack\n",
        "\n",
        "    guards = state.get(\"guards\", {})\n",
        "    if not evidence_pack:\n",
        "        guards[\"evidence_insufficient\"] = True\n",
        "        guards.setdefault(\"notes\", []).append(\"Extractor: evidence_pack is empty.\")\n",
        "    state[\"guards\"] = guards\n",
        "\n",
        "    logs.append({\"node\":\"extractor\", \"t\": time.time()-t0, \"questions\": len(evidence_pack)})\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "m7UmSOv0cKlA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "MARKET_MODEL_PROMPT_KR_V1 = \"\"\"역할: (대한민국 한정) evidence_pack과 market_plan을 이용해 Scale-up(C) 분석을 만든다.\n",
        "출력(JSON only):\n",
        "{\n",
        "  \"market_scope\":\"...\",\n",
        "  \"tam_sam_som\": {\n",
        "    \"tam_krw_range\":{\"min\":0,\"base\":0,\"max\":0,\"basis\":\"EVIDENCE|ASSUMPTION\",\"notes\":\"...\"},\n",
        "    \"sam_krw_range\":{\"min\":0,\"base\":0,\"max\":0,\"basis\":\"EVIDENCE|ASSUMPTION\",\"notes\":\"...\"},\n",
        "    \"som_krw_range\":{\"min\":0,\"base\":0,\"max\":0,\"basis\":\"EVIDENCE|ASSUMPTION\",\"notes\":\"...\"}\n",
        "  },\n",
        "  \"cagr\":{\n",
        "    \"scenario_pct\":{\"conservative\":0.0,\"base\":0.0,\"aggressive\":0.0,\"basis\":\"EVIDENCE|ASSUMPTION\"},\n",
        "    \"how_estimated\":\"...\"\n",
        "  },\n",
        "  \"ecosystem\":{\n",
        "    \"taxonomy\":[\"...\"],\n",
        "    \"firms\":[{\"name\":\"...\",\"category\":\"...\",\"size_hint\":\"대|중|소|스타트업|공공\",\"evidence_url\":\"\"}],\n",
        "    \"distribution_summary\":\"...\",\n",
        "    \"concentration_risk\":\"...\"\n",
        "  },\n",
        "  \"risks\":[\"...\"],\n",
        "  \"evidence_links\":[\"...\"],\n",
        "  \"confidence\":{\"market_size\":0.0,\"cagr\":0.0,\"ecosystem\":0.0}\n",
        "}\n",
        "규칙:\n",
        "- 범위는 대한민국. 해외/글로벌 데이터 금지.\n",
        "- evidence_links는 evidence_pack.meta.url에서만 선택(최소 3개).\n",
        "- 숫자는 evidence_pack에 근거가 없으면 0으로 두고 basis를 ASSUMPTION으로 표시 + notes/how_estimated에 검증 방법을 써라.\n",
        "- firms는 최대 25개. 중복 제거.\"\"\"\n",
        "market_model_agent = Agent(\n",
        "    name=\"MarketModelKR\",\n",
        "    instructions=MARKET_MODEL_PROMPT_KR_V1,\n",
        "    tools=[],\n",
        ")\n",
        "\n",
        "def market_model_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    payload = {\n",
        "        \"idea_schema\": state.get(\"idea_schema\", {}),\n",
        "        \"market_plan\": state.get(\"market_plan\", {}),\n",
        "        \"evidence_pack\": state.get(\"evidence_pack\", []),\n",
        "    }\n",
        "    messages = [{\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}]\n",
        "    out = run_agent(messages, market_model_agent)\n",
        "    data = safe_json_loads(out)\n",
        "\n",
        "    obj = MarketModelModel(**data)\n",
        "    state[\"market_model\"] = obj.model_dump()\n",
        "\n",
        "    logs.append({\"node\":\"market_model\", \"t\": time.time()-t0})\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n",
        "\n",
        "\n",
        "COST_PLAN_PROMPT_KR_V1 = \"\"\"역할: 아이디어만으로 '최소비용 MVP 개발 계획(B)'을 설계하고, 시뮬레이션 가능한 입력(3점 추정/비용 범위)을 만든다.\n",
        "출력(JSON only):\n",
        "{\n",
        "  \"mvp_scope\":{\"must\":[...],\"should\":[...],\"could\":[...]},\n",
        "  \"wbs\":{\n",
        "    \"milestones\":[\n",
        "      {\"id\":\"M1\",\"name\":\"...\",\"exit_criteria\":[\"...\"],\"is_validation_gate\":false},\n",
        "      {\"id\":\"M2\",\"name\":\"...\",\"exit_criteria\":[\"...\"],\"is_validation_gate\":true}\n",
        "    ],\n",
        "    \"work_packages\":[\n",
        "      {\"id\":\"WP1\",\"milestone_id\":\"M1\",\"name\":\"...\",\"deliverable\":\"...\",\"stage\":\"foundation|mvp|validation|nice_to_have\",\n",
        "       \"dependencies\":[\"WP0\"],\"roles\":[\"PM\",\"BE\",\"FE\",\"ML\",\"DS\",\"DESIGN\"],\n",
        "       \"effort_days_pert\":{\"optimistic\":1.0,\"most_likely\":2.0,\"pessimistic\":4.0},\n",
        "       \"risk_notes\":[\"...\"]\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  \"validation_plan\":{\n",
        "    \"hypotheses\":[{\"id\":\"H1\",\"statement\":\"...\",\"risk_if_false\":\"...\"}],\n",
        "    \"experiments\":[{\"id\":\"E1\",\"goal\":\"...\",\"method\":\"...\",\"metric\":\"...\",\"success_threshold\":\"...\",\"min_sample\":\"...\",\"milestone_id\":\"M2\"}]\n",
        "  },\n",
        "  \"cost_model_assumptions\":{\n",
        "    \"role_day_rate_krw_range\":{\n",
        "      \"PM\":{\"min\":0,\"base\":0,\"max\":0},\n",
        "      \"BE\":{\"min\":0,\"base\":0,\"max\":0},\n",
        "      \"FE\":{\"min\":0,\"base\":0,\"max\":0},\n",
        "      \"ML\":{\"min\":0,\"base\":0,\"max\":0},\n",
        "      \"DS\":{\"min\":0,\"base\":0,\"max\":0},\n",
        "      \"DESIGN\":{\"min\":0,\"base\":0,\"max\":0}\n",
        "    },\n",
        "    \"infra_monthly_krw_range\":{\"min\":0,\"base\":0,\"max\":0},\n",
        "    \"llm_api_monthly_krw_range\":{\"min\":0,\"base\":0,\"max\":0},\n",
        "    \"data_purchase_oneoff_krw_range\":{\"min\":0,\"base\":0,\"max\":0},\n",
        "    \"overhead_pct_range\":{\"min\":0.0,\"base\":0.0,\"max\":0.0}\n",
        "  },\n",
        "  \"assumptions_and_unknowns\":[{\"type\":\"ASSUMPTION\",\"statement\":\"...\",\"impact\":\"...\",\"how_to_verify\":\"...\"}]\n",
        "}\n",
        "규칙:\n",
        "- 대한민국 창업/개발 관점에서 현실적인 숫자 범위를 제시하되, 전부 '가정'임을 assumptions_and_unknowns에 명시해라.\n",
        "- work_packages는 12~25개 수준으로. dependencies는 DAG(순환 금지).\n",
        "- is_validation_gate=true 인 milestone은 1개만(검증 완료 게이트).\n",
        "- stage는 반드시 지정.\n",
        "- effort_days_pert는 person-day 기준.\"\"\"\n",
        "cost_plan_agent = Agent(\n",
        "    name=\"CostPlanKR\",\n",
        "    instructions=COST_PLAN_PROMPT_KR_V1,\n",
        "    tools=[],\n",
        ")\n",
        "\n",
        "def _sample_pert(a: float, m: float, b: float, size: int, lamb: float = 4.0) -> np.ndarray:\n",
        "    a, m, b = float(a), float(m), float(b)\n",
        "    if b <= a:\n",
        "        return np.full(size, a, dtype=float)\n",
        "    # beta-PERT\n",
        "    alpha = 1.0 + lamb * (m - a) / (b - a)\n",
        "    beta = 1.0 + lamb * (b - m) / (b - a)\n",
        "    x = np.random.beta(alpha, beta, size=size)\n",
        "    return a + x * (b - a)\n",
        "\n",
        "def _sample_tri(min_v: float, mode_v: float, max_v: float, size: int) -> np.ndarray:\n",
        "    min_v, mode_v, max_v = float(min_v), float(mode_v), float(max_v)\n",
        "    if max_v <= min_v:\n",
        "        return np.full(size, min_v, dtype=float)\n",
        "    mode_v = min(max(mode_v, min_v), max_v)\n",
        "    return np.random.triangular(min_v, mode_v, max_v, size=size)\n",
        "\n",
        "def _topo_order(nodes: List[str], deps: Dict[str, List[str]]) -> List[str]:\n",
        "    # Kahn\n",
        "    indeg = {n: 0 for n in nodes}\n",
        "    adj = {n: [] for n in nodes}\n",
        "    for n in nodes:\n",
        "        for d in deps.get(n, []) or []:\n",
        "            if d in indeg:\n",
        "                adj[d].append(n)\n",
        "                indeg[n] += 1\n",
        "    q = [n for n in nodes if indeg[n] == 0]\n",
        "    out = []\n",
        "    while q:\n",
        "        n = q.pop(0)\n",
        "        out.append(n)\n",
        "        for nxt in adj.get(n, []):\n",
        "            indeg[nxt] -= 1\n",
        "            if indeg[nxt] == 0:\n",
        "                q.append(nxt)\n",
        "    if len(out) != len(nodes):\n",
        "        # cycle fallback: keep given order\n",
        "        return nodes\n",
        "    return out\n",
        "\n",
        "def _simulate_cost_plan(plan: Dict[str, Any], iterations: int = 5000, seed: int = 42) -> Dict[str, Any]:\n",
        "    np.random.seed(seed)\n",
        "    wbs = (plan or {}).get(\"wbs\", {}) or {}\n",
        "    wps = wbs.get(\"work_packages\", []) or []\n",
        "\n",
        "    # Build WP maps\n",
        "    wp_ids = []\n",
        "    deps = {}\n",
        "    roles_by_wp = {}\n",
        "    pert_by_wp = {}\n",
        "    milestone_by_wp = {}\n",
        "    for wp in wps:\n",
        "        wid = wp.get(\"id\")\n",
        "        if not wid:\n",
        "            continue\n",
        "        wp_ids.append(wid)\n",
        "        deps[wid] = wp.get(\"dependencies\", []) or []\n",
        "        roles_by_wp[wid] = wp.get(\"roles\", []) or []\n",
        "        e = (wp.get(\"effort_days_pert\", {}) or {})\n",
        "        pert_by_wp[wid] = (e.get(\"optimistic\", 1.0), e.get(\"most_likely\", 2.0), e.get(\"pessimistic\", 4.0))\n",
        "        milestone_by_wp[wid] = wp.get(\"milestone_id\",\"\")\n",
        "\n",
        "    wp_ids = list(dict.fromkeys(wp_ids))\n",
        "    order = _topo_order(wp_ids, deps)\n",
        "\n",
        "    # Sample durations\n",
        "    dur = np.zeros((iterations, len(wp_ids)), dtype=float)\n",
        "    id_to_idx = {wid:i for i,wid in enumerate(wp_ids)}\n",
        "    for wid, (a,m,b) in pert_by_wp.items():\n",
        "        i = id_to_idx.get(wid)\n",
        "        if i is None:\n",
        "            continue\n",
        "        dur[:, i] = _sample_pert(a,m,b,iterations)\n",
        "\n",
        "    # Schedule (earliest finish)\n",
        "    ef = np.zeros_like(dur)\n",
        "    for wid in order:\n",
        "        i = id_to_idx[wid]\n",
        "        dep_ids = [d for d in deps.get(wid, []) or [] if d in id_to_idx]\n",
        "        if not dep_ids:\n",
        "            ef[:, i] = dur[:, i]\n",
        "        else:\n",
        "            dep_idx = [id_to_idx[d] for d in dep_ids]\n",
        "            ef[:, i] = dur[:, i] + np.max(ef[:, dep_idx], axis=1)\n",
        "    total_effort_days = np.sum(dur, axis=1)\n",
        "    project_duration_days = np.max(ef, axis=1)  # dependency-aware\n",
        "\n",
        "    # Cost\n",
        "    ass = (plan or {}).get(\"cost_model_assumptions\", {}) or {}\n",
        "    role_rates = ass.get(\"role_day_rate_krw_range\", {}) or {}\n",
        "    # sample one rate per role per iteration\n",
        "    role_rate_samples = {}\n",
        "    for role, r in role_rates.items():\n",
        "        if not isinstance(r, dict):\n",
        "            continue\n",
        "        role_rate_samples[role] = _sample_tri(r.get(\"min\",0), r.get(\"base\",0), r.get(\"max\",0), iterations)\n",
        "    # labor cost\n",
        "    labor = np.zeros(iterations, dtype=float)\n",
        "    for wid in wp_ids:\n",
        "        i = id_to_idx[wid]\n",
        "        roles = roles_by_wp.get(wid, []) or []\n",
        "        if not roles:\n",
        "            continue\n",
        "        rates = []\n",
        "        for role in roles:\n",
        "            if role in role_rate_samples:\n",
        "                rates.append(role_rate_samples[role])\n",
        "        if not rates:\n",
        "            continue\n",
        "        avg_rate = np.mean(np.vstack(rates), axis=0)\n",
        "        labor += dur[:, i] * avg_rate\n",
        "\n",
        "    overhead = ass.get(\"overhead_pct_range\", {}) or {}\n",
        "    overhead_pct = _sample_tri(overhead.get(\"min\",0.0), overhead.get(\"base\",0.0), overhead.get(\"max\",0.0), iterations)\n",
        "    labor_with_overhead = labor * (1.0 + overhead_pct)\n",
        "\n",
        "    # recurring\n",
        "    def _monthly_cost(key: str) -> np.ndarray:\n",
        "        r = ass.get(key, {}) or {}\n",
        "        return _sample_tri(r.get(\"min\",0), r.get(\"base\",0), r.get(\"max\",0), iterations)\n",
        "\n",
        "    months = project_duration_days / 20.0\n",
        "    infra = months * _monthly_cost(\"infra_monthly_krw_range\")\n",
        "    llm = months * _monthly_cost(\"llm_api_monthly_krw_range\")\n",
        "\n",
        "    data_oneoff_r = ass.get(\"data_purchase_oneoff_krw_range\", {}) or {}\n",
        "    data_oneoff = _sample_tri(data_oneoff_r.get(\"min\",0), data_oneoff_r.get(\"base\",0), data_oneoff_r.get(\"max\",0), iterations)\n",
        "\n",
        "    total_cost = labor_with_overhead + infra + llm + data_oneoff\n",
        "\n",
        "    def pct(x, p):\n",
        "        return float(np.percentile(x, p))\n",
        "\n",
        "    return {\n",
        "        \"iterations\": iterations,\n",
        "        \"duration_days\": {\"p50\": pct(project_duration_days,50), \"p80\": pct(project_duration_days,80), \"mean\": float(np.mean(project_duration_days))},\n",
        "        \"effort_person_days\": {\"p50\": pct(total_effort_days,50), \"p80\": pct(total_effort_days,80), \"mean\": float(np.mean(total_effort_days))},\n",
        "        \"cost_krw\": {\"p50\": pct(total_cost,50), \"p80\": pct(total_cost,80), \"mean\": float(np.mean(total_cost))},\n",
        "    }\n",
        "\n",
        "def _subset_plan_until_validation_gate(plan: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    wbs = (plan or {}).get(\"wbs\", {}) or {}\n",
        "    milestones = wbs.get(\"milestones\", []) or []\n",
        "    wps = wbs.get(\"work_packages\", []) or []\n",
        "\n",
        "    gate_id = None\n",
        "    for ms in milestones:\n",
        "        if ms.get(\"is_validation_gate\") is True:\n",
        "            gate_id = ms.get(\"id\")\n",
        "            break\n",
        "    if not gate_id:\n",
        "        return plan\n",
        "\n",
        "    # keep milestones up to gate\n",
        "    ms_ids = [ms.get(\"id\") for ms in milestones if ms.get(\"id\")]\n",
        "    try:\n",
        "        gate_idx = ms_ids.index(gate_id)\n",
        "        keep_ms = set(ms_ids[:gate_idx+1])\n",
        "    except ValueError:\n",
        "        keep_ms = set(ms_ids)\n",
        "\n",
        "    keep_wps = [wp for wp in wps if wp.get(\"milestone_id\") in keep_ms]\n",
        "    # also include dependencies closure\n",
        "    wp_map = {wp.get(\"id\"): wp for wp in wps if wp.get(\"id\")}\n",
        "    keep_set = set([wp.get(\"id\") for wp in keep_wps if wp.get(\"id\")])\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        for wid in list(keep_set):\n",
        "            wp = wp_map.get(wid, {})\n",
        "            for d in (wp.get(\"dependencies\", []) or []):\n",
        "                if d and d in wp_map and d not in keep_set:\n",
        "                    keep_set.add(d)\n",
        "                    changed = True\n",
        "    keep_wps = [wp_map[wid] for wid in keep_set if wid in wp_map]\n",
        "\n",
        "    new_plan = dict(plan)\n",
        "    new_wbs = dict(wbs)\n",
        "    new_wbs[\"work_packages\"] = keep_wps\n",
        "    new_wbs[\"milestones\"] = [ms for ms in milestones if ms.get(\"id\") in keep_ms]\n",
        "    new_plan[\"wbs\"] = new_wbs\n",
        "    return new_plan\n",
        "\n",
        "def cost_sim_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    payload = {\n",
        "        \"idea_schema\": state.get(\"idea_schema\", {}),\n",
        "        \"market_model\": state.get(\"market_model\", {}),\n",
        "    }\n",
        "    messages = [{\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}]\n",
        "    out = run_agent(messages, cost_plan_agent)\n",
        "    plan = safe_json_loads(out)\n",
        "\n",
        "    # main simulation\n",
        "    sim = _simulate_cost_plan(plan, iterations=5000, seed=42)\n",
        "\n",
        "    # min-to-validate simulation\n",
        "    sub_plan = _subset_plan_until_validation_gate(plan)\n",
        "    sim_min = _simulate_cost_plan(sub_plan, iterations=3000, seed=43)\n",
        "\n",
        "    cost_sim = {\n",
        "        \"mvp_scope\": plan.get(\"mvp_scope\", {}),\n",
        "        \"wbs\": plan.get(\"wbs\", {}),\n",
        "        \"validation_plan\": plan.get(\"validation_plan\", {}),\n",
        "        \"cost_model_assumptions\": plan.get(\"cost_model_assumptions\", {}),\n",
        "        \"simulation\": sim,\n",
        "        \"min_to_validate\": {\"plan_hint\": \"until validation gate milestone\", \"simulation\": sim_min},\n",
        "        \"assumptions_and_unknowns\": plan.get(\"assumptions_and_unknowns\", []),\n",
        "    }\n",
        "\n",
        "    obj = CostSimModel(**cost_sim)\n",
        "    state[\"cost_sim\"] = obj.model_dump()\n",
        "\n",
        "    logs.append({\"node\":\"cost_sim\", \"t\": time.time()-t0})\n",
        "    state[\"logs\"] = logs\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "uVrIc5-JcLcJ"
      },
      "outputs": [],
      "source": [
        "ANALYSIS_PROMPT = \"\"\"\n",
        "역할: evidence_pack을 기반으로 시장/경쟁/고객/리스크 신호를 요약하고,\n",
        "설명가능한 점수(0~5)를 만든다.\n",
        "출력(JSON only):\n",
        "{\n",
        "  \"market\": \"...\",\n",
        "  \"competition\": \"...\",\n",
        "  \"customer\": \"...\",\n",
        "  \"risks\": \"...\",\n",
        "  \"score_explainable\": {\"market\":3.0,\"competition\":2.5,\"customer\":3.5,\"risks\":2.0}\n",
        "}\n",
        "규칙:\n",
        "- fact / interpretation / hypothesis를 문장 앞 라벨로 구분해라.\n",
        "- 수치(성장률 등)는 evidence_pack에 근거가 없으면 생성하지 마라.\n",
        "\"\"\"\n",
        "\n",
        "analysis_agent = Agent(name=\"Analysis\", instructions=ANALYSIS_PROMPT, tools=[])\n",
        "\n",
        "def analysis_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    payload = {\n",
        "        \"idea_schema\": state[\"idea_schema\"],\n",
        "        \"evidence_pack\": state.get(\"evidence_pack\", []),\n",
        "        \"market_plan\": state.get(\"market_plan\", {}),\n",
        "        \"market_model\": state.get(\"market_model\", {}),\n",
        "        \"cost_sim\": state.get(\"cost_sim\", {})\n",
        "    }\n",
        "    messages = [{\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}]\n",
        "    out = run_agent(messages, analysis_agent)\n",
        "    data = safe_json_loads(out)\n",
        "\n",
        "    obj = SignalsModel(**data)\n",
        "    state[\"signals\"] = obj.model_dump()\n",
        "\n",
        "    logs.append({\"node\":\"analysis\", \"t\": time.time()-t0, \"score\": obj.score_explainable})\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "NmzkwlircM_1"
      },
      "outputs": [],
      "source": [
        "DECISION_PROMPT = \"\"\"\n",
        "역할: signals + market_model + cost_sim + evidence_pack을 바탕으로\n",
        "GO / NO_GO / PIVOT 결론을 내리고, 근거 링크와 다음 액션(검증 실험 포함)을 제안한다.\n",
        "출력(JSON only):\n",
        "{\n",
        "  \"decision\":\"GO|NO_GO|PIVOT\",\n",
        "  \"key_reasons\":[\"...\"],\n",
        "  \"evidence_links\":[\"...\"],\n",
        "  \"next_actions\":[\"...\"]\n",
        "}\n",
        "규칙:\n",
        "- evidence_links는 evidence_pack.meta.url에서만 가져와라(최소 3개).\n",
        "- 이유(key_reasons)에 market_model(시장규모/TAM-SAM-SOM, CAGR, 생태계/경쟁구도) 요소를 최소 1개 포함.\n",
        "- 이유 또는 액션(next_actions)에 cost_sim(일정/비용 p50·p80, min_to_validate 범위) 요소를 최소 1개 포함.\n",
        "- 단정 금지: 불확실하면 PIVOT 또는 조건부 GO로 표현.\n",
        "\"\"\"\n",
        "\n",
        "decision_agent = Agent(name=\"Decision\", instructions=DECISION_PROMPT, tools=[])\n",
        "\n",
        "def decision_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    payload = {\n",
        "        \"idea_schema\": state[\"idea_schema\"],\n",
        "        \"signals\": state.get(\"signals\", {}),\n",
        "        \"evidence_pack\": state.get(\"evidence_pack\", []),\n",
        "        \"market_plan\": state.get(\"market_plan\", {}),\n",
        "        \"market_model\": state.get(\"market_model\", {}),\n",
        "        \"cost_sim\": state.get(\"cost_sim\", {})\n",
        "    }\n",
        "    messages = [{\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}]\n",
        "    out = run_agent(messages, decision_agent)\n",
        "    data = safe_json_loads(out)\n",
        "\n",
        "    obj = VerdictModel(**data)\n",
        "    state[\"verdict\"] = obj.model_dump()\n",
        "\n",
        "    logs.append({\"node\":\"decision\", \"t\": time.time()-t0, \"decision\": obj.decision})\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n",
        "\n",
        "def route_after_decision(state: WorkflowState) -> str:\n",
        "    return \"blueprint\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "50BU9iW3cPzA"
      },
      "outputs": [],
      "source": [
        "BLUEPRINT_PROMPT = '''\n",
        "역할: verdict를 반영하여 MVP 설계 산출물을 만든다.\n",
        "출력(JSON only):\n",
        "{\n",
        "  \"prd_1p\": \"<markdown string>\",\n",
        "  \"scope_must_should_could\": \"<markdown string>\",\n",
        "  \"erd_mermaid\": \"```mermaid ...```\",\n",
        "  \"user_flow\": \"<markdown string>\",\n",
        "  \"roadmap_2_4_weeks\": \"<markdown string>\",\n",
        "  \"validation_plan\": \"<markdown string>\"\n",
        "}\n",
        "규칙:\n",
        "- 위 6개 필드는 '문자열'이어야 한다. (객체/리스트 JSON으로 내지 말 것)\n",
        "- scope/roadmap/validation은 사람이 읽기 좋은 bullet markdown으로 작성.\n",
        "- ERD는 Mermaid ER diagram 또는 flowchart 형식.\n",
        "- 검증 플랜은 '실험-지표-판정기준'이 포함되어야 함.\n",
        "'''\n",
        "blueprint_agent = Agent(\n",
        "    name=\"BlueprintMaker\",\n",
        "    instructions=BLUEPRINT_PROMPT,\n",
        "    tools=[],\n",
        ")\n",
        "\n",
        "def _to_markdown(x: Any, indent: int = 0) -> str:\n",
        "    pad = \"  \" * indent\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "    if isinstance(x, str):\n",
        "        return x.strip()\n",
        "    if isinstance(x, list):\n",
        "        lines = []\n",
        "        for item in x:\n",
        "            if isinstance(item, (dict, list)):\n",
        "                lines.append(f\"{pad}-\")\n",
        "                child = _to_markdown(item, indent+1)\n",
        "                if child:\n",
        "                    lines.append(child)\n",
        "            else:\n",
        "                lines.append(f\"{pad}- {str(item)}\")\n",
        "        return \"\\n\".join(lines).strip()\n",
        "    if isinstance(x, dict):\n",
        "        lines = []\n",
        "        for k, v in x.items():\n",
        "            if isinstance(v, (dict, list)):\n",
        "                lines.append(f\"{pad}- **{k}**\")\n",
        "                child = _to_markdown(v, indent+1)\n",
        "                if child:\n",
        "                    lines.append(child)\n",
        "            else:\n",
        "                lines.append(f\"{pad}- **{k}**: {str(v)}\")\n",
        "        return \"\\n\".join(lines).strip()\n",
        "    return str(x).strip()\n",
        "\n",
        "def blueprint_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    payload = {\n",
        "        \"idea_schema\": state[\"idea_schema\"],\n",
        "        \"signals\": state.get(\"signals\", {}),\n",
        "        \"verdict\": state.get(\"verdict\", {})\n",
        "    }\n",
        "    messages = [{\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}]\n",
        "    out = run_agent(messages, blueprint_agent)\n",
        "    data = safe_json_loads(out)\n",
        "\n",
        "    for key in [\"prd_1p\", \"scope_must_should_could\", \"erd_mermaid\", \"user_flow\", \"roadmap_2_4_weeks\", \"validation_plan\"]:\n",
        "        if key in data and not isinstance(data[key], str):\n",
        "            data[key] = _to_markdown(data[key])\n",
        "\n",
        "    obj = ArtifactsModel(**data)\n",
        "    state[\"artifacts\"] = obj.model_dump()\n",
        "\n",
        "    logs.append({\"node\":\"blueprint\", \"t\": time.time()-t0})\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "XrgZgafjcRcx"
      },
      "outputs": [],
      "source": [
        "GUARDRAIL_PROMPT = \"\"\"\n",
        "역할: 최종 산출물(analysis/verdict/artifacts)이 아래 가드를 충족하는지 점검하고,\n",
        "위반/부족 플래그를 설정하며, 필요한 최소 수정(라벨/단정 표현 완화/링크 누락 보완)을 제안한다.\n",
        "출력(JSON only):\n",
        "{\n",
        "  \"policy_violation\": false,\n",
        "  \"token_overflow\": false,\n",
        "  \"copyright_risk\": false,\n",
        "  \"evidence_insufficient\": false,\n",
        "  \"notes\": [\"...\"]\n",
        "}\n",
        "체크리스트:\n",
        "- 핵심 주장에 출처 링크가 최소 3개 이상인가?\n",
        "- fact/interpretation/hypothesis 라벨이 존재하는가?\n",
        "- 수치가 '근거 없이' 생성되지 않았는가?\n",
        "- 뉴스/리포트 전문을 길게 인용하지 않았는가?\n",
        "\"\"\"\n",
        "\n",
        "guardrail_agent = Agent(name=\"GuardrailValidator\", instructions=GUARDRAIL_PROMPT, tools=[])\n",
        "\n",
        "def guardrail_node(state: WorkflowState) -> WorkflowState:\n",
        "    logs = state.get(\"logs\", [])\n",
        "    t0 = time.time()\n",
        "\n",
        "    payload = {\n",
        "        \"signals\": state.get(\"signals\", {}),\n",
        "        \"verdict\": state.get(\"verdict\", {}),\n",
        "        \"artifacts\": state.get(\"artifacts\", {}),\n",
        "        \"evidence_pack\": state.get(\"evidence_pack\", [])\n",
        "    }\n",
        "    messages = [{\"role\":\"user\",\"content\": json.dumps(payload, ensure_ascii=False)}]\n",
        "    out = run_agent(messages, guardrail_agent)\n",
        "    data = safe_json_loads(out)\n",
        "\n",
        "    obj = GuardsModel(**data)\n",
        "    state[\"guards\"] = obj.model_dump()\n",
        "\n",
        "    logs.append({\"node\":\"guardrail\", \"t\": time.time()-t0, \"flags\": {\n",
        "        \"policy_violation\": obj.policy_violation,\n",
        "        \"token_overflow\": obj.token_overflow,\n",
        "        \"copyright_risk\": obj.copyright_risk,\n",
        "        \"evidence_insufficient\": obj.evidence_insufficient\n",
        "    }})\n",
        "    state[\"logs\"] = logs\n",
        "    return state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "G_TwvK2OcTKY"
      },
      "outputs": [],
      "source": [
        "def render_report(state: WorkflowState) -> WorkflowState:\n",
        "    idea = state.get(\"idea_schema\", {}) or {}\n",
        "    signals = state.get(\"signals\", {}) or {}\n",
        "    verdict = state.get(\"verdict\", {}) or {}\n",
        "    artifacts = state.get(\"artifacts\", {}) or {}\n",
        "    guards = state.get(\"guards\", {}) or {}\n",
        "    market_model = state.get(\"market_model\", {}) or {}\n",
        "    cost_sim = state.get(\"cost_sim\", {}) or {}\n",
        "    mp = state.get(\"market_plan\", {}) or {}\n",
        "\n",
        "    def _money(v):\n",
        "        try:\n",
        "            v = float(v)\n",
        "        except Exception:\n",
        "            return str(v)\n",
        "        if v >= 1e12:\n",
        "            return f\"{v/1e12:.2f}조원\"\n",
        "        if v >= 1e8:\n",
        "            return f\"{v/1e8:.2f}억원\"\n",
        "        if v >= 1e4:\n",
        "            return f\"{v/1e4:.1f}만원\"\n",
        "        return f\"{v:.0f}원\"\n",
        "\n",
        "    # Evidence links (unique, order-preserving)\n",
        "    links = []\n",
        "    for ep in (state.get(\"evidence_pack\", []) or []):\n",
        "        for ch in (ep.get(\"top_chunks\", []) or []):\n",
        "            url = ((ch.get(\"meta\", {}) or {}).get(\"url\",\"\") or \"\").strip()\n",
        "            if url and url not in links:\n",
        "                links.append(url)\n",
        "    # also include verdict links\n",
        "    for u in (verdict.get(\"evidence_links\", []) or []):\n",
        "        if u and u not in links:\n",
        "            links.append(u)\n",
        "\n",
        "    out = []\n",
        "    out.append(\"# IdeaProof 리포트 (KR)\\n\")\n",
        "\n",
        "    # --- Problem ---\n",
        "    out.append(\"## Problem\\n\")\n",
        "    out.append(f\"- **문제(요약):** {idea.get('problem','')}\\n\")\n",
        "    out.append(f\"- **타겟(가정):** {idea.get('target','')}\\n\")\n",
        "    if signals:\n",
        "        if signals.get(\"market\"): out.append(f\"- **시장 신호:** {signals.get('market')}\\n\")\n",
        "        if signals.get(\"customer\"): out.append(f\"- **고객 신호:** {signals.get('customer')}\\n\")\n",
        "        if signals.get(\"risks\"): out.append(f\"- **리스크 신호:** {signals.get('risks')}\\n\")\n",
        "\n",
        "    # --- Solution ---\n",
        "    out.append(\"\\n## Solution\\n\")\n",
        "    out.append(f\"- **해결책(요약):** {idea.get('solution','')}\\n\")\n",
        "    out.append(f\"- **차별점:** {idea.get('differentiation','')}\\n\")\n",
        "    out.append(f\"- **BM(수익모델):** {idea.get('business_model','')}\\n\")\n",
        "    if artifacts:\n",
        "        if artifacts.get(\"mvp_scope\"):\n",
        "            out.append(\"\\n### MVP Scope\\n\")\n",
        "            out.append(artifacts.get(\"mvp_scope\",\"\")+\"\\n\")\n",
        "        if artifacts.get(\"user_flow\"):\n",
        "            out.append(\"\\n### User Flow\\n\")\n",
        "            out.append(artifacts.get(\"user_flow\",\"\")+\"\\n\")\n",
        "        if artifacts.get(\"architecture\"):\n",
        "            out.append(\"\\n### Architecture\\n\")\n",
        "            out.append(artifacts.get(\"architecture\",\"\")+\"\\n\")\n",
        "\n",
        "    # --- Scale-up ---\n",
        "    out.append(\"\\n## Scale-up\\n\")\n",
        "\n",
        "    # C: market model\n",
        "    out.append(\"\\n### C) 거시→미시→생태계 분석 (대한민국)\\n\")\n",
        "    if market_model.get(\"market_scope\"):\n",
        "        out.append(f\"- **시장 범위:** {market_model.get('market_scope')}\\n\")\n",
        "    tss = market_model.get(\"tam_sam_som\", {}) or {}\n",
        "    if tss:\n",
        "        out.append(\"- **TAM/SAM/SOM(범위):**\\n\")\n",
        "        for k,label in [(\"tam_krw_range\",\"TAM\"),(\"sam_krw_range\",\"SAM\"),(\"som_krw_range\",\"SOM\")]:\n",
        "            r = tss.get(k, {}) or {}\n",
        "            if r:\n",
        "                out.append(f\"  - {label}: {_money(r.get('min',0))} ~ {_money(r.get('max',0))} (base {_money(r.get('base',0))}, {r.get('basis','')})\\n\")\n",
        "    cagr = market_model.get(\"cagr\", {}) or {}\n",
        "    if cagr.get(\"scenario_pct\"):\n",
        "        sc = cagr.get(\"scenario_pct\", {}) or {}\n",
        "        out.append(f\"- **CAGR 시나리오(%):** 보수 {sc.get('conservative')} / 기준 {sc.get('base')} / 공격 {sc.get('aggressive')} ({sc.get('basis','')})\\n\")\n",
        "    eco = market_model.get(\"ecosystem\", {}) or {}\n",
        "    firms = eco.get(\"firms\", []) or []\n",
        "    if firms:\n",
        "        out.append(f\"- **생태계 기업(최대 25):** {', '.join([f.get('name','') for f in firms if f.get('name')][:25])}\\n\")\n",
        "    if eco.get(\"distribution_summary\"):\n",
        "        out.append(f\"- **분포 요약:** {eco.get('distribution_summary')}\\n\")\n",
        "    if market_model.get(\"risks\"):\n",
        "        out.append(\"- **Scale-up 리스크:**\\n\")\n",
        "        for r in market_model.get(\"risks\", [])[:8]:\n",
        "            out.append(f\"  - {r}\\n\")\n",
        "\n",
        "    # B: cost & schedule simulation\n",
        "    out.append(\"\\n### B) 최소비용 MVP 개발계획 + 시뮬레이션(가정)\\n\")\n",
        "    sim = (cost_sim.get(\"simulation\", {}) or {})\n",
        "    if sim:\n",
        "        dur = sim.get(\"duration_days\", {}) or {}\n",
        "        cost = sim.get(\"cost_krw\", {}) or {}\n",
        "        eff = sim.get(\"effort_person_days\", {}) or {}\n",
        "        out.append(f\"- **예상 일정(의존성 반영):** p50 {dur.get('p50')}일 / p80 {dur.get('p80')}일\\n\")\n",
        "        out.append(f\"- **예상 노력(총 person-day):** p50 {eff.get('p50')} / p80 {eff.get('p80')}\\n\")\n",
        "        out.append(f\"- **예상 비용(총):** p50 {_money(cost.get('p50',0))} / p80 {_money(cost.get('p80',0))}\\n\")\n",
        "    minv = (cost_sim.get(\"min_to_validate\", {}) or {}).get(\"simulation\", {}) or {}\n",
        "    if minv:\n",
        "        dur = minv.get(\"duration_days\", {}) or {}\n",
        "        cost = minv.get(\"cost_krw\", {}) or {}\n",
        "        out.append(f\"- **검증 완료(게이트)까지 최소 범위:** p50 {dur.get('p50')}일, p80 {dur.get('p80')}일 / 비용 p50 {_money(cost.get('p50',0))}\\n\")\n",
        "    wbs = cost_sim.get(\"wbs\", {}) or {}\n",
        "    if wbs.get(\"milestones\"):\n",
        "        out.append(\"\\n#### Milestones\\n\")\n",
        "        for ms in wbs.get(\"milestones\", [])[:10]:\n",
        "            gate = \" (VALIDATION GATE)\" if ms.get(\"is_validation_gate\") else \"\"\n",
        "            out.append(f\"- {ms.get('id')}: {ms.get('name')}{gate}\\n\")\n",
        "    if wbs.get(\"work_packages\"):\n",
        "        out.append(\"\\n#### Work Packages (Top)\\n\")\n",
        "        for wp in wbs.get(\"work_packages\", [])[:12]:\n",
        "            e = wp.get(\"effort_days_pert\", {}) or {}\n",
        "            out.append(f\"- {wp.get('id')}: {wp.get('name')} [{wp.get('stage')}] (PERT {e.get('optimistic')}/{e.get('most_likely')}/{e.get('pessimistic')}d)\\n\")\n",
        "\n",
        "    # Decision inside Scale-up\n",
        "    out.append(\"\\n### 판정 (GO / NO_GO / PIVOT)\\n\")\n",
        "    out.append(f\"- **결론:** {verdict.get('decision','')}\\n\")\n",
        "    if verdict.get(\"key_reasons\"):\n",
        "        out.append(\"- **핵심 근거:**\\n\")\n",
        "        for r in verdict.get(\"key_reasons\", [])[:8]:\n",
        "            out.append(f\"  - {r}\\n\")\n",
        "    if verdict.get(\"next_actions\"):\n",
        "        out.append(\"- **다음 액션:**\\n\")\n",
        "        for a in verdict.get(\"next_actions\", [])[:10]:\n",
        "            out.append(f\"  - {a}\\n\")\n",
        "\n",
        "    # Evidence and guardrail as subsections (still within Scale-up)\n",
        "    if links:\n",
        "        out.append(\"\\n### 근거 링크(Top)\\n\")\n",
        "        for u in links[:12]:\n",
        "            out.append(f\"- {u}\\n\")\n",
        "\n",
        "    if guards:\n",
        "        out.append(\"\\n### Guardrail\\n\")\n",
        "        out.append(\"```json\\n\"+json.dumps(guards, ensure_ascii=False, indent=2)+\"\\n```\\n\")\n",
        "\n",
        "    # logs\n",
        "    logs = state.get(\"logs\", [])\n",
        "    if logs:\n",
        "        out.append(\"\\n### Logs (Tail)\\n\")\n",
        "        out.append(\"```json\\n\"+json.dumps(logs[-20:], ensure_ascii=False, indent=2)+\"\\n```\\n\")\n",
        "\n",
        "    state[\"final_report_markdown\"] = \"\\n\".join(out)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5Hmhz-KTcT6n"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "graph = StateGraph(WorkflowState)\n",
        "\n",
        "# --- nodes ---\n",
        "graph.add_node(\"intake\", intake_node)\n",
        "graph.add_node(\"structurer\", structurer_node)\n",
        "\n",
        "# Group 2 (C + B)\n",
        "graph.add_node(\"market_plan\", market_plan_node)\n",
        "graph.add_node(\"evidence_builder\", evidence_builder_node)\n",
        "graph.add_node(\"extractor\", extractor_node)\n",
        "graph.add_node(\"market_model\", market_model_node)\n",
        "graph.add_node(\"cost_sim\", cost_sim_node)\n",
        "\n",
        "graph.add_node(\"analysis\", analysis_node)\n",
        "graph.add_node(\"decision\", decision_node)\n",
        "graph.add_node(\"blueprint\", blueprint_node)\n",
        "graph.add_node(\"guardrail\", guardrail_node)\n",
        "graph.add_node(\"render\", render_report)\n",
        "\n",
        "# --- edges ---\n",
        "graph.add_edge(START, \"intake\")\n",
        "graph.add_conditional_edges(\n",
        "    \"intake\",\n",
        "    route_after_intake,\n",
        "    {\n",
        "        \"chat_end\": END,\n",
        "        \"structurer\": \"structurer\",\n",
        "    },\n",
        ")\n",
        "\n",
        "graph.add_edge(\"structurer\", \"market_plan\")\n",
        "graph.add_edge(\"market_plan\", \"evidence_builder\")\n",
        "graph.add_edge(\"evidence_builder\", \"extractor\")\n",
        "graph.add_edge(\"extractor\", \"market_model\")\n",
        "graph.add_edge(\"market_model\", \"cost_sim\")\n",
        "graph.add_edge(\"cost_sim\", \"analysis\")\n",
        "graph.add_edge(\"analysis\", \"decision\")\n",
        "\n",
        "# decision -> blueprint (always for now, but keep as conditional hook)\n",
        "graph.add_conditional_edges(\"decision\", route_after_decision, {\"blueprint\": \"blueprint\"})\n",
        "graph.add_edge(\"blueprint\", \"guardrail\")\n",
        "graph.add_edge(\"guardrail\", \"render\")\n",
        "graph.add_edge(\"render\", END)\n",
        "\n",
        "app = graph.compile()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "F9L6TTKzczkA",
        "outputId": "e0b0b9a1-0547-4a4c-ff28-2971441ef9da"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\n",
              "    \u001b[32m'problem'\u001b[0m: \u001b[32m'창업 아이디어 검증 과정의 비체계성, 시장 신호 수집의 어려움, MVP 설계의 전문성 부족으로 인한 실패 \u001b[0m\n",
              "\u001b[32m리스크 증가'\u001b[0m,\n",
              "    \u001b[32m'target'\u001b[0m: \u001b[32m'초기 창업자, 스타트업 팀, 예비 창업자, 벤처 캐피탈의 초기 단계 투자 심사 담당자'\u001b[0m,\n",
              "    \u001b[32m'solution'\u001b[0m: \u001b[32m'AI 기반 창업 아이디어 검증 플랫폼으로, 산업 분류 → 데이터 기반 시장/경쟁/고객 분석 → 객관적 검증 \u001b[0m\n",
              "\u001b[32m프로세스 → 실행 가능한 피벗 제안 및 MVP 설계 산출물 패키지 제공'\u001b[0m,\n",
              "    \u001b[32m'differentiation'\u001b[0m: \u001b[32m'1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 자동화된 산업 분류 및 신호 수집 시스템 2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 검증된 프레임워크 기반의 의사결정 지원 3\u001b[0m\u001b[32m)\u001b[0m\u001b[32m MVP \u001b[0m\n",
              "\u001b[32m설계\u001b[0m\u001b[32m(\u001b[0m\u001b[32mERD/로드맵 등\u001b[0m\u001b[32m)\u001b[0m\u001b[32m의 즉시 생성 기능 4\u001b[0m\u001b[32m)\u001b[0m\u001b[32m 피벗 제안의 데이터 기반 접근'\u001b[0m,\n",
              "    \u001b[32m'business_model'\u001b[0m: \u001b[32m'SaaS 구독 모델\u001b[0m\u001b[32m(\u001b[0m\u001b[32mBasic/Pro/Enterprise\u001b[0m\u001b[32m)\u001b[0m\u001b[32m + 프리미엄 검증 리포트 유료 판매 + 액셀러레이터 \u001b[0m\n",
              "\u001b[32m파트너십 수수료'\u001b[0m,\n",
              "    \u001b[32m'industry'\u001b[0m: \u001b[32m'스타트업 인큐베이팅/액셀러레이팅, SaaS, AI 기반 의사결정 지원 도구'\u001b[0m,\n",
              "    \u001b[32m'keywords'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[32m'창업 검증'\u001b[0m,\n",
              "        \u001b[32m'스타트업 MVP 설계'\u001b[0m,\n",
              "        \u001b[32m'시장 신호 분석'\u001b[0m,\n",
              "        \u001b[32m'AI 기반 의사결정'\u001b[0m,\n",
              "        \u001b[32m'피벗 전략'\u001b[0m,\n",
              "        \u001b[32m'ERD 생성'\u001b[0m,\n",
              "        \u001b[32m'로드맵 자동화'\u001b[0m,\n",
              "        \u001b[32m'창업 실패 리스크 감소'\u001b[0m\n",
              "    \u001b[1m]\u001b[0m,\n",
              "    \u001b[32m'persona_hypotheses'\u001b[0m: \u001b[1m[\u001b[0m\n",
              "        \u001b[32m'초기 창업자는 검증 과정의 전문성 부족으로 인해 서비스 의존도가 높을 것'\u001b[0m,\n",
              "        \u001b[32m'액셀러레이터는 포트폴리오 회사들의 검증 효율화를 위해 기업용 플랜에 관심을 가질 것'\u001b[0m\n",
              "    \u001b[1m]\u001b[0m\n",
              "\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'problem'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'창업 아이디어 검증 과정의 비체계성, 시장 신호 수집의 어려움, MVP 설계의 전문성 부족으로 인한 실패 </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">리스크 증가'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'target'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'초기 창업자, 스타트업 팀, 예비 창업자, 벤처 캐피탈의 초기 단계 투자 심사 담당자'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'solution'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'AI 기반 창업 아이디어 검증 플랫폼으로, 산업 분류 → 데이터 기반 시장/경쟁/고객 분석 → 객관적 검증 </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">프로세스 → 실행 가능한 피벗 제안 및 MVP 설계 산출물 패키지 제공'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'differentiation'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'1) 자동화된 산업 분류 및 신호 수집 시스템 2) 검증된 프레임워크 기반의 의사결정 지원 3) MVP </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">설계(ERD/로드맵 등)의 즉시 생성 기능 4) 피벗 제안의 데이터 기반 접근'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'business_model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SaaS 구독 모델(Basic/Pro/Enterprise) + 프리미엄 검증 리포트 유료 판매 + 액셀러레이터 </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">파트너십 수수료'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'industry'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'스타트업 인큐베이팅/액셀러레이팅, SaaS, AI 기반 의사결정 지원 도구'</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'keywords'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'창업 검증'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'스타트업 MVP 설계'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'시장 신호 분석'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'AI 기반 의사결정'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'피벗 전략'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'ERD 생성'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'로드맵 자동화'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'창업 실패 리스크 감소'</span>\n",
              "    <span style=\"font-weight: bold\">]</span>,\n",
              "    <span style=\"color: #008000; text-decoration-color: #008000\">'persona_hypotheses'</span>: <span style=\"font-weight: bold\">[</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'초기 창업자는 검증 과정의 전문성 부족으로 인해 서비스 의존도가 높을 것'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'액셀러레이터는 포트폴리오 회사들의 검증 효율화를 위해 기업용 플랜에 관심을 가질 것'</span>\n",
              "    <span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0m\n",
              "    \u001b[1m{\u001b[0m\u001b[32m'node'\u001b[0m: \u001b[32m'intake'\u001b[0m, \u001b[32m't'\u001b[0m: \u001b[1;36m3.7074832916259766\u001b[0m, \u001b[32m'route'\u001b[0m: \u001b[32m'workflow'\u001b[0m\u001b[1m}\u001b[0m,\n",
              "    \u001b[1m{\u001b[0m\n",
              "        \u001b[32m'node'\u001b[0m: \u001b[32m'structurer'\u001b[0m,\n",
              "        \u001b[32m't'\u001b[0m: \u001b[1;36m2.9190919399261475\u001b[0m,\n",
              "        \u001b[32m'industry'\u001b[0m: \u001b[32m'스타트업 인큐베이팅/액셀러레이팅, SaaS, AI 기반 의사결정 지원 도구'\u001b[0m\n",
              "    \u001b[1m}\u001b[0m\n",
              "\u001b[1m]\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
              "    <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'node'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'intake'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.7074832916259766</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'route'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'workflow'</span><span style=\"font-weight: bold\">}</span>,\n",
              "    <span style=\"font-weight: bold\">{</span>\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'node'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'structurer'</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'t'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.9190919399261475</span>,\n",
              "        <span style=\"color: #008000; text-decoration-color: #008000\">'industry'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'스타트업 인큐베이팅/액셀러레이팅, SaaS, AI 기반 의사결정 지원 도구'</span>\n",
              "    <span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">]</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 아이디어 입력: \"기획서 자체\"를 아이디어로 사용 (사용자 요구사항 반영)\n",
        "IDEA_TEXT = \"\"\"\n",
        "IdeaProof(가칭)는 창업 아이디어를 입력하면 산업 분류 → 시장/경쟁/고객 신호 수집 → 근거 기반 검증 →\n",
        "Go/No-Go/Pivot 결론을 제공하고, 결론에 따라 피벗 제안과 MVP 설계 산출물(ERD, 로드맵, 검증 플랜)을\n",
        "패키지로 생성하는 서비스이다.\n",
        "\"\"\".strip()\n",
        "\n",
        "state0: WorkflowState = {\n",
        "    \"request\": {\"raw_request\": IDEA_TEXT, \"language\": \"ko\", \"tone\": \"concise\", \"mode\": \"standard\"},\n",
        "    \"logs\": []\n",
        "}\n",
        "\n",
        "# 예: intake + structurer만 단독 테스트\n",
        "tmp = intake_node(state0.copy())\n",
        "tmp = structurer_node(tmp)\n",
        "rprint(tmp[\"idea_schema\"])\n",
        "rprint(tmp[\"logs\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib9yufkCc8tc",
        "outputId": "fe44518b-1bd2-4f73-b961-f7c7ada1d9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOG] {'node': 'intake', 't': 2.864471912384033, 'route': 'workflow'}\n",
            "[LOG] {'node': 'structurer', 't': 3.85493540763855, 'industry': 'AI SaaS, 스타트업 지원 서비스, 비즈니스 인텔리전스, 창업 교육 플랫폼'}\n",
            "[LOG] {'node': 'market_plan', 't': 14.844451665878296, 'layers': 4}\n",
            "[LOG] {'node': 'evidence_builder', 't': 237.807373046875, 'cache': 'MISS', 'collection': 'ideaproof_e224f797096a9bd7', 'mode': 'standard', 'sources': 50, 'stored_chunks': 67, 'time_budget_sec': 720, 'time_budget_hit': False}\n",
            "[LOG] {'node': 'extractor', 't': 29.479384899139404, 'questions': 4}\n",
            "[LOG] {'node': 'market_model', 't': 13.484107971191406}\n",
            "[LOG] {'node': 'cost_sim', 't': 18.11776614189148}\n",
            "[LOG] {'node': 'analysis', 't': 4.254802465438843, 'score': {'market': 3.0, 'competition': 2.5, 'customer': 3.5, 'risks': 2.0}}\n",
            "[LOG] {'node': 'decision', 't': 7.119373798370361, 'decision': 'PIVOT'}\n",
            "[LOG] {'node': 'blueprint', 't': 8.88239312171936}\n",
            "[LOG] {'node': 'guardrail', 't': 3.179896593093872, 'flags': {'policy_violation': False, 'token_overflow': False, 'copyright_risk': False, 'evidence_insufficient': False}}\n",
            "\n",
            "\n",
            "================ FINAL REPORT (markdown) ================\n",
            "\n",
            "# IdeaProof 리포트 (KR)\n",
            "\n",
            "## Problem\n",
            "\n",
            "- **문제(요약):** 예비 창업자가 아이디어 검증 과정에 체계적인 방법론과 데이터 부족으로 인해 불필요한 시간과 자원을 낭비함. 시장 수요, 경쟁 구도, 고객 니즈에 대한 객관적 근거 없이 직관이나 경험에 의존해 실패 리스크가 높음\n",
            "\n",
            "- **타겟(가정):** 예비 창업자, 스타트업 팀, 대학 창업 동아리, 사내 벤처 팀, 액셀러레이터/인큐베이터 프로그램 참여자\n",
            "\n",
            "- **시장 신호:** fact: 국내 창업 지원 시장 규모는 2023년 기준 1.2~1.5조원 범위로 추정됨(국회예산정책처, 중소벤처기업부 보고서). interpretation: 정부의 TIPS 프로그램 등 정책적 지원이 시장 성장을 주도하며, 연간 10~20% CAGR이 예상됨. hypothesis: AI SaaS 기반 검증 도구 수요는 디지털 전환 투자 증가와 함께 지속 확대될 전망.\n",
            "\n",
            "- **고객 신호:** fact: 예비 창업자의 28.8%가 아이디어 검증/프로토타입 제작을 가장 시급한 지원 항목으로 요구(중소상공인진흥공단 보고서). interpretation: 복잡한 데이터 분석보다 직관적인 시각화와 실행 가능한 액션 플랜에 대한 니즈가 높음. hypothesis: 생성형 AI 시뮬레이션 정확도와 사용자 경험(UX) 개선이 고객 확보의 핵심 요소.\n",
            "\n",
            "- **리스크 신호:** fact: AI 데이터 크롤링 규제 리스크 및 생성형 AI 시뮬레이션 신뢰도 문제 존재(개인정보보호위원회 가이드라인). interpretation: 기술적·법적 장벽이 진입 장애로 작용할 수 있음. hypothesis: 공공 데이터 활용 및 법적 검토 강화로 리스크 완화 가능.\n",
            "\n",
            "\n",
            "## Solution\n",
            "\n",
            "- **해결책(요약):** AI 분석을 통해 입력된 창업 아이디어에 대해 산업 분류/시장 신호/경쟁 분석/고객 피드백을 자동 수집하고, 데이터 기반 검증 리포트를 생성. 결과에 따라 사업 추진 방향(Go/No-Go/Pivot)을 제시하며, 필요 시 피벗 제안과 MVP 개발을 위한 ERD/로드맵/검증 플랜을 패키지로 제공\n",
            "\n",
            "- **차별점:** 기존 아이디어 검증 서비스가 단순 체크리스트나 컨설턴트 주관에 의존하는 반면, IdeaProof는 실시간 시장 데이터 크롤링 + 경쟁사 분석 + 가상 고객 시뮬레이션(생성 AI)을 결합한 자동 검증 시스템. MVP 설계 산출물까지 원스톱으로 제공하는 것이 차별화 포인트\n",
            "\n",
            "- **BM(수익모델):** 1) 기본 검증 서비스: 월정액 구독제 (스타트업/개인 대상) 2) 프리미엄 패키지: MVP 설계 포함 1회성 결제 (중소벤처/기업 사내벤처 대상) 3) B2B API: 액셀러레이터/인큐베이터에 화이트라벨 솔루션 공급 4) 데이터 마켓플레이스: 검증된 산업별 트렌드 리포트 판매\n",
            "\n",
            "\n",
            "### User Flow\n",
            "\n",
            "1. **아이디어 제출**: 사용자가 산업 분류/핵심 기능/타겟 고객 입력\n",
            "2. **자동 분석**: 시스템이 3가지 AI 모듈로 병렬 분석 수행\n",
            "   - 시장 신호 크롤러: 정부/금융/뉴스 데이터 스캔\n",
            "   - 경쟁 분석기: 유사 제품/경쟁사 특허/가격 데이터 수집\n",
            "   - 가상 고객 시뮬레이션: 생성형 AI로 1,000명 이상의 가상 피드백 생성\n",
            "3. **결과 리포트**: 24시간 내 50+ 데이터 포인트를 포함한 검증 리포트 발송\n",
            "4. **의사 결정**: AI 판정 결과(Go/No-Go/Pivot)와 액션 플랜 제시\n",
            "5. **피벗 지원**: Pivot 시 MVP 설계 산출물 패키지 제공\n",
            "\n",
            "\n",
            "## Scale-up\n",
            "\n",
            "\n",
            "### C) 거시→미시→생태계 분석 (대한민국)\n",
            "\n",
            "- **시장 범위:** 대한민국 창업 생태계 내 AI 기반 아이디어 검증 및 MVP 설계 자동화 SaaS 서비스\n",
            "\n",
            "- **TAM/SAM/SOM(범위):**\n",
            "\n",
            "  - TAM: 0원 ~ 150.00억원 (base 120.00억원, ASSUMPTION)\n",
            "\n",
            "  - SAM: 0원 ~ 30.00억원 (base 24.00억원, ASSUMPTION)\n",
            "\n",
            "  - SOM: 0원 ~ 8.00억원 (base 6.00억원, ASSUMPTION)\n",
            "\n",
            "- **CAGR 시나리오(%):** 보수 10.0 / 기준 15.0 / 공격 20.0 (ASSUMPTION)\n",
            "\n",
            "- **생태계 기업(최대 25):** LG CNS, SK C&C, 사이냅소프트, D.CAMP, 창업진흥원\n",
            "\n",
            "- **분포 요약:** 대기업(LG CNS, SK C&C)이 AI SaaS 시장을 주도하며, 중견기업(사이냅소프트) 및 액셀러레이터(D.CAMP)가 니치 시장 공략 중. 창업진흥원 등 공공기관이 예산 지원을 통해 생태계 활성화.\n",
            "\n",
            "- **Scale-up 리스크:**\n",
            "\n",
            "  - AI 데이터 크롤링 관련 규제 리스크(개인정보보호위원회 가이드라인 미준수 시 과태료 부과)\n",
            "\n",
            "  - 대기업과의 기술 격차 및 마케팅 예산 부족으로 인한 진입 장벽\n",
            "\n",
            "  - 창업자의 가격 민감도로 인한 SaaS 전환율 저조 가능성\n",
            "\n",
            "  - 생성형 AI 시뮬레이션의 정확도에 대한 신뢰도 문제\n",
            "\n",
            "\n",
            "### B) 최소비용 MVP 개발계획 + 시뮬레이션(가정)\n",
            "\n",
            "- **예상 일정(의존성 반영):** p50 54.4481239611402일 / p80 57.97026903233829일\n",
            "\n",
            "- **예상 노력(총 person-day):** p50 61.59621650010418 / p80 65.20772677095448\n",
            "\n",
            "- **예상 비용(총):** p50 1.70억원 / p80 1.93억원\n",
            "\n",
            "- **검증 완료(게이트)까지 최소 범위:** p50 54.33222361010012일, p80 57.85790395565762일 / 비용 p50 1.70억원\n",
            "\n",
            "\n",
            "#### Milestones\n",
            "\n",
            "- M1: 검증 시스템 프로토타입 개발\n",
            "\n",
            "- M2: 검증 시스템 베타 테스트 (VALIDATION GATE)\n",
            "\n",
            "\n",
            "#### Work Packages (Top)\n",
            "\n",
            "- WP1: 요구사항 분석 및 기술 검토 [foundation] (PERT 2/3/5d)\n",
            "\n",
            "- WP2: 아이디어 입력 폼 개발 [foundation] (PERT 5/7/10d)\n",
            "\n",
            "- WP3: 시장 데이터 크롤링 시스템 구축 [foundation] (PERT 10/15/25d)\n",
            "\n",
            "- WP4: 기본 리포트 생성 엔진 개발 [mvp] (PERT 8/10/15d)\n",
            "\n",
            "- WP5: 가상 고객 시뮬레이션 개발 [validation] (PERT 12/16/24d)\n",
            "\n",
            "- WP6: 검증 시스템 테스트 [validation] (PERT 6/8/12d)\n",
            "\n",
            "\n",
            "### 판정 (GO / NO_GO / PIVOT)\n",
            "\n",
            "- **결론:** PIVOT\n",
            "\n",
            "- **핵심 근거:**\n",
            "\n",
            "  - 국내 창업 지원 시장 규모는 1.2~1.5조원(TAM)으로 성장 잠재력이 높으나, 대기업(LG CNS, SK C&C)의 시장 집중도가 높아 중소기업 진입 장벽이 존재함(market_model.ecosystem.firms, https://www.khan.co.kr/article/202310241648001)\n",
            "\n",
            "  - 예비 창업자의 28.8%가 아이디어 검증을 가장 시급한 지원 항목으로 요구하나(customer.fact), 생성형 AI 시뮬레이션 정확도와 UX 개선이 고객 확보의 핵심 요소로 불확실함(customer.hypothesis, https://www.mss.go.kr/cmm/fms/FileDown.do?atchFileId=FILE_000000001007895&fileSn=1)\n",
            "\n",
            "  - 검증 시스템 개발에 p50 기준 1.7억 원(170,344,415원)이 소요되며(cost_sim.cost_krw.p50), 초기 검증 단계(min_to_validate) 비용만 1.6억 원 이상 필요해 자원 효율성 검토가 필요함\n",
            "\n",
            "- **다음 액션:**\n",
            "\n",
            "  - 액셀러레이터(D.CAMP) 및 창업진흥원과의 파트너십을 통한 화이트라벨 솔루션 공급 모델 검증(ecosystem_firms.goals, https://www.k-aia.or.kr/home_action/d_filedownload_v/559/bbsinfo/1)\n",
            "\n",
            "  - 생성형 AI 시뮬레이션 정확도 검증을 위한 실험(E2) 수행: 10개 아이디어 대상으로 실제 시장 데이터와 생성 결과 비교 분석(validation_pla\n"
          ]
        }
      ],
      "source": [
        "# 실행 + 진행상황(노드별) 출력: 어디서 오래 걸리는지 바로 보이게 함\n",
        "state_init: WorkflowState = {\"request\": {\"raw_request\": IDEA_TEXT, \"language\":\"ko\", \"tone\":\"concise\", \"mode\":\"standard\"}, \"logs\":[]}\n",
        "\n",
        "last_log_len = 0\n",
        "final_state = None\n",
        "\n",
        "for s in app.stream(state_init, stream_mode=\"values\"):\n",
        "    logs = s.get(\"logs\", [])\n",
        "    if len(logs) > last_log_len:\n",
        "        for item in logs[last_log_len:]:\n",
        "            print(\"[LOG]\", item)\n",
        "        last_log_len = len(logs)\n",
        "    final_state = s\n",
        "\n",
        "print(\"\\n\\n================ FINAL REPORT (markdown) ================\\n\")\n",
        "print((final_state or {}).get(\"final_report_markdown\",\"\")[:4000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADys9EynjPwr",
        "outputId": "0c4901d7-5d15-43ca-ed6c-a68603cab5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved: ideaproof_report.md\n"
          ]
        }
      ],
      "source": [
        "out_path = \"ideaproof_report.md\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(final_state.get(\"final_report_markdown\",\"\"))\n",
        "print(\"saved:\", out_path)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8bdf38cf2f894b4e9b99c1e76cd2c4dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ffa42d9a8c04e0ebf6b32faf3499249",
              "IPY_MODEL_f15d7c3364c640b5802dcd7fd51a1077",
              "IPY_MODEL_cc316a8081504ccba2b83fd1279c72a6"
            ],
            "layout": "IPY_MODEL_7b1fc055802548f6ae62daa869ff22f2"
          }
        },
        "9ffa42d9a8c04e0ebf6b32faf3499249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a61c8336c2974892be036797e8c06bf2",
            "placeholder": "​",
            "style": "IPY_MODEL_ce1e16479a2f4bc781aff839fbf22e28",
            "value": "tokenizer_config.json: "
          }
        },
        "f15d7c3364c640b5802dcd7fd51a1077": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41af3d55107a460586458be83dc35588",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47778ab100c84431a79905af78fd26e3",
            "value": 1
          }
        },
        "cc316a8081504ccba2b83fd1279c72a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1257b01f47d44fa19fe12fa46f362382",
            "placeholder": "​",
            "style": "IPY_MODEL_6810648e6bd4446d891fc55e390b9698",
            "value": " 24.6k/? [00:00&lt;00:00, 1.19MB/s]"
          }
        },
        "7b1fc055802548f6ae62daa869ff22f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a61c8336c2974892be036797e8c06bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce1e16479a2f4bc781aff839fbf22e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41af3d55107a460586458be83dc35588": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "47778ab100c84431a79905af78fd26e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1257b01f47d44fa19fe12fa46f362382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6810648e6bd4446d891fc55e390b9698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "106387fd8fb642fd981b560a975f5893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_050fa80c0510407a8e97a64970e7b15d",
              "IPY_MODEL_599bb2b216f24a62b09239dbf70c5304",
              "IPY_MODEL_2f9349df7e6649c583675a9ac3f7b929"
            ],
            "layout": "IPY_MODEL_73e4bf219dd74b2d8edf2931edb2a993"
          }
        },
        "050fa80c0510407a8e97a64970e7b15d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c029e0fcc9b4548857dda04d697e44b",
            "placeholder": "​",
            "style": "IPY_MODEL_fa5c56c7a16c42e3aebc4e093ea2c8cb",
            "value": "tokenizer.model: 100%"
          }
        },
        "599bb2b216f24a62b09239dbf70c5304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fd969898b3143bebb47c441d67d261c",
            "max": 499744,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e552a8507d947e3a8cb203168734eda",
            "value": 499744
          }
        },
        "2f9349df7e6649c583675a9ac3f7b929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2cad387267245ef80ce6d93537c61f6",
            "placeholder": "​",
            "style": "IPY_MODEL_520eeb66026d439087022f75440ed023",
            "value": " 500k/500k [00:00&lt;00:00, 628kB/s]"
          }
        },
        "73e4bf219dd74b2d8edf2931edb2a993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c029e0fcc9b4548857dda04d697e44b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa5c56c7a16c42e3aebc4e093ea2c8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fd969898b3143bebb47c441d67d261c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e552a8507d947e3a8cb203168734eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2cad387267245ef80ce6d93537c61f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "520eeb66026d439087022f75440ed023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "291767cbc913437095f499c9d65d0aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0ad2d6f952147b9b718f2f0e7e503fe",
              "IPY_MODEL_07f9c93dcd7147b9991c72e1435caaad",
              "IPY_MODEL_f46bd4e68e274c57a3d4932f2a31fb56"
            ],
            "layout": "IPY_MODEL_11a7b7e22bf64b7cab99c023ad7863a0"
          }
        },
        "b0ad2d6f952147b9b718f2f0e7e503fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e6088d8ad5f4dfa9a68c016bccc514a",
            "placeholder": "​",
            "style": "IPY_MODEL_fb77cbc6b55d4a05ae47c0430d5743de",
            "value": "tokenizer.json: "
          }
        },
        "07f9c93dcd7147b9991c72e1435caaad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9419e481d774662af7919cd0ee2cf5a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4b2df44258b463ab2d05a40f58f9ac6",
            "value": 1
          }
        },
        "f46bd4e68e274c57a3d4932f2a31fb56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea67c00cc6c840c2bc52364640fcf42a",
            "placeholder": "​",
            "style": "IPY_MODEL_b27235c9e5de43158f99753f40329865",
            "value": " 1.87M/? [00:00&lt;00:00, 26.1MB/s]"
          }
        },
        "11a7b7e22bf64b7cab99c023ad7863a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e6088d8ad5f4dfa9a68c016bccc514a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb77cbc6b55d4a05ae47c0430d5743de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9419e481d774662af7919cd0ee2cf5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b4b2df44258b463ab2d05a40f58f9ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea67c00cc6c840c2bc52364640fcf42a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b27235c9e5de43158f99753f40329865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a0f5dfdede5434e9599dd97fb993374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_619c6ce072644ade91d1583fa69aef07",
              "IPY_MODEL_edd0431253234612b2374d4e6359b479",
              "IPY_MODEL_48a4c3f78ad34aa3a490fa571cd119d7"
            ],
            "layout": "IPY_MODEL_c52e7f326cd24114ac4525f72cd5b5ed"
          }
        },
        "619c6ce072644ade91d1583fa69aef07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d52049b153f34e71bac569f797a7a541",
            "placeholder": "​",
            "style": "IPY_MODEL_5b2808fc518045239977a851b14c6d24",
            "value": "added_tokens.json: "
          }
        },
        "edd0431253234612b2374d4e6359b479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82a741dba2c34d848d15e9bf65b24fbd",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1274498cd6c4c25bc5f7672aa34790d",
            "value": 1
          }
        },
        "48a4c3f78ad34aa3a490fa571cd119d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_559748f2c9fd4ee6ab379a0145859b83",
            "placeholder": "​",
            "style": "IPY_MODEL_b1869747fe734fb2b6441e8950bd3519",
            "value": " 3.83k/? [00:00&lt;00:00, 293kB/s]"
          }
        },
        "c52e7f326cd24114ac4525f72cd5b5ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52049b153f34e71bac569f797a7a541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b2808fc518045239977a851b14c6d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82a741dba2c34d848d15e9bf65b24fbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a1274498cd6c4c25bc5f7672aa34790d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "559748f2c9fd4ee6ab379a0145859b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1869747fe734fb2b6441e8950bd3519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02b40eceb6ff4c7e8b864d1f1b31d213": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81ed446696884dbebd3256e1eb82b1d0",
              "IPY_MODEL_885efc1add764621b687abb001479b72",
              "IPY_MODEL_43487b9ac7ec4c69b1632155b4f427ee"
            ],
            "layout": "IPY_MODEL_5c242ab75e944b9394ef7bb01a120439"
          }
        },
        "81ed446696884dbebd3256e1eb82b1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d81f41cac8cb432bbf5117198f119d8c",
            "placeholder": "​",
            "style": "IPY_MODEL_4b014a87c5bc46a28da41e8573451d31",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "885efc1add764621b687abb001479b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a6f91edb59a4075ab37cbfa485c0292",
            "max": 575,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cfa9e44224e542b8a8582d8842fcd5a3",
            "value": 575
          }
        },
        "43487b9ac7ec4c69b1632155b4f427ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c26d68b3dc7244a2b6952e40fafed87c",
            "placeholder": "​",
            "style": "IPY_MODEL_3a9bfd7d43b14a9cbbdb13606e8a5f90",
            "value": " 575/575 [00:00&lt;00:00, 38.5kB/s]"
          }
        },
        "5c242ab75e944b9394ef7bb01a120439": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d81f41cac8cb432bbf5117198f119d8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b014a87c5bc46a28da41e8573451d31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a6f91edb59a4075ab37cbfa485c0292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa9e44224e542b8a8582d8842fcd5a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c26d68b3dc7244a2b6952e40fafed87c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a9bfd7d43b14a9cbbdb13606e8a5f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}